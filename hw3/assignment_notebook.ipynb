{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU 10-405/10-605 auto-graded notebook\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Logistic Regression & PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe60cd9540617cc2e184126ec775bb7a",
     "grade": false,
     "grade_id": "collaborators",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment? \n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "# collaborators = [\"\"]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dccedd0e8702e7a99d0ea08f2fa7921",
     "grade": true,
     "grade_id": "test_collaborators",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click-Through Rate Prediction\n",
    "In this section you will go through the steps for creating a click-through rate (CTR) prediction pipeline. You will work with the Criteo Labs dataset.\n",
    "\n",
    "## This section will cover:\n",
    "\n",
    "* *Part 1:* Featurize categorical data using one-hot-encoding (OHE)\n",
    "\n",
    "* *Part 2:* Construct an OHE dictionary\n",
    "\n",
    "* *Part 3:* Parse CTR data and generate OHE features\n",
    " * *Visualization 1:* Feature frequency\n",
    "\n",
    "* *Part 4:* CTR prediction and logloss evaluation\n",
    " * *Visualization 2:* ROC curve\n",
    "\n",
    "* *Part 5:* Reduce feature dimension via feature hashing\n",
    "\n",
    "> Note that, for reference, you can look up the details of:\n",
    "> * the relevant Spark methods in [PySpark's DataFrame API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.DataFrame)\n",
    "> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN MOST LIKELY IGNORE THIS CELL. This is only of use for running this notebook locally.\n",
    "\n",
    "# THIS CELL DOES NOT NEED TO BE RUN ON DATABRICKS. \n",
    "# Note that Databricks already creates a SparkContext for you, so this cell can be skipped.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext(appName=\"hw\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"spark context started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Featurize categorical data using one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) One-hot-encoding\n",
    "\n",
    "We would like to develop code to convert categorical features to numerical ones. In order to build intuition, we will work with a unlabeled dataset with three data points, with each data point representing an animal. The first feature indicates the type of animal (bear, cat, mouse); the second feature describes the animal's color (black, tabby); and the third (optional) feature describes what the animal eats (mouse, salmon).\n",
    "\n",
    "In a one-hot-encoding (OHE) scheme, we want to represent each tuple of `(featureID, category)` via its own binary feature.  We can do this in Python by creating a dictionary that maps each tuple to a distinct integer, where the integer corresponds to a binary feature. To start, manually enter the entries in the OHE dictionary associated with the sample dataset by mapping the tuples to consecutive integers starting from zero,  ordering the tuples first by featureID and next by category.\n",
    "\n",
    "Later in this lab, we'll use OHE dictionaries to transform data points into compact lists of features that can be used in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, when a shuffle operation occurs with DataFrames, the post-shuffle partition\n",
    "# count is 200. This is controlled by Spark configuration value spark.sql.shuffle.partitions.\n",
    "# 200 is a little too high for this data set, so we set the post-shuffle partition count to\n",
    "# twice the number of available threads in Community Edition.\n",
    "sqlContext.setConf('spark.sql.shuffle.partitions', '6')  # Set default partitions for DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Data for manual OHE\n",
    "# Note: the first data point does not include any value for the optional third feature\n",
    "sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "\n",
    "def sample_to_row(sample):\n",
    "    tmp_dict = defaultdict(lambda: None)\n",
    "    tmp_dict.update(sample)\n",
    "    return [tmp_dict[i] for i in range(3)]\n",
    "\n",
    "sqlContext.createDataFrame(map(sample_to_row, [sample_one, sample_two, sample_three]),\n",
    "                           ['animal', 'color', 'food']).show()\n",
    "sample_data_df = sqlContext.createDataFrame([(sample_one,), (sample_two,), (sample_three,)], ['features'])\n",
    "sample_data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8209ea16dbf40e73e55ee82ff619d7c1",
     "grade": false,
     "grade_id": "answer_oneHotEncoding1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Replace <FILL IN> with appropriate code\n",
    "# sample_ohe_dict_manual = {}\n",
    "# sample_ohe_dict_manual[(0, 'bear')] = <FILL IN >\n",
    "# sample_ohe_dict_manual[(0, 'cat')] = <FILL IN >\n",
    "# sample_ohe_dict_manual[(0, 'mouse')] = <FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "# sample_ohe_dict_manual < FILL IN >\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6f25c59f4552f4f7f88adb22f436d4",
     "grade": true,
     "grade_id": "test_oneHotEncoding1a",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST One-hot-encoding (1a)\n",
    "assert_equal(sample_ohe_dict_manual[(0, 'bear')], 0)\n",
    "assert_equal(sample_ohe_dict_manual[(0, 'cat')], 1)\n",
    "assert_equal(sample_ohe_dict_manual[(0, 'mouse')], 2)\n",
    "assert_equal(len(sample_ohe_dict_manual.keys()), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Sparse vectors\n",
    "\n",
    "Data points can typically be represented with a small number of non-zero OHE features which are relative to the total number of features that occur in the dataset.  By leveraging this sparsity and using sparse vector representations for OHE data, we can reduce storage and computational burdens.  Below are a few sample vectors represented as dense numpy arrays.  Use [SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) to represent them in a sparse fashion, and verify that both the sparse and dense representations yield the same results when computing [dot products](http://en.wikipedia.org/wiki/Dot_product) (we will later use MLlib to train classifiers via gradient descent, and MLlib will need to compute dot products between SparseVectors and dense parameter vectors).\n",
    "\n",
    "Use `SparseVector(size, *args)` to create a new sparse vector where size is the length of the vector and args are either:\n",
    "1. A list of indices and a list of values corresponding to the indices. The indices list must be sorted in ascending order. For example, SparseVector(5, [1, 3, 4], [10, 30, 40]) will represent the vector [0, 10, 0, 30, 40]. The non-zero indices are 1, 3 and 4. On the other hand, SparseVector(3, [2, 1], [5, 5]) will give you an error because the indices list [2, 1] is not in ascending order. Note: you cannot simply sort the indices list, because otherwise the values will not correspond to the respective indices anymore.\n",
    "2. A list of (index, value) pair. In this case, the indices need not be sorted. For example, SparseVector(5, [(3, 1), (1, 2)]) will give you the vector [0, 2, 0, 1, 0].\n",
    "\n",
    "SparseVectors are much more efficient when working with sparse data because they do not store zero values (only store non-zero values and their indices). You'll need to create a sparse vector representation of each dense vector `a_dense` and `b_dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f4aab6e45b9f308eac40fed53799b29",
     "grade": false,
     "grade_id": "answer_sparseVector1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# a_dense = np.array([0., 3., 0., 4.])\n",
    "# a_sparse = <FILL IN >\n",
    "\n",
    "# b_dense = np.array([0., 0., 0., 1.])\n",
    "# b_sparse = <FILL IN >\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "w = np.array([0.4, 3.1, -1.4, -.5])\n",
    "print(a_dense.dot(w))\n",
    "print(a_sparse.dot(w))\n",
    "print(b_dense.dot(w))\n",
    "print(b_sparse.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f77449dbd8d1d2dbc73e796e8fb2298e",
     "grade": true,
     "grade_id": "test_sparseVector1b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sparse Vectors (1b)\n",
    "assert_true(isinstance(a_sparse, SparseVector), 'a_sparse needs to be an instance of SparseVector')\n",
    "assert_true(b_dense.dot(w) == b_sparse.dot(w),\n",
    "                'dot product of b_dense and w should equal dot product of b_sparse and w')\n",
    "assert_true(a_sparse.numNonzeros() == 2, 'a_sparse should not store zero values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) OHE features as sparse vectors\n",
    "\n",
    "Now let's see how we can represent the OHE features for points in our sample dataset.  Using the mapping defined by the OHE dictionary from Part (1a), manually define OHE features for the three sample data points using SparseVector format.  In this case, all the features will have a value of 1.0.  For example, the `DenseVector` for a point with features 2 and 4 would be `[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of the sample features\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3370e32345c2352fb4238ac5b882b69c",
     "grade": false,
     "grade_id": "answer_oheFeature1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code. Use SparseVector\n",
    "# sample_one_ohe_feat_manual = <FILL IN >\n",
    "# sample_two_ohe_feat_manual = <FILL IN >\n",
    "# sample_three_ohe_feat_manual = <FILL IN >\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36983ed66139e8e07fd2526ca7983d04",
     "grade": true,
     "grade_id": "test_oheFeature1c",
     "locked": true,
     "points": 11,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST OHE Features as sparse vectors (1c)\n",
    "assert_true(isinstance(sample_one_ohe_feat_manual, SparseVector),\n",
    "                'sample_one_ohe_feat_manual needs to be a SparseVector')\n",
    "assert_true(isinstance(sample_two_ohe_feat_manual, SparseVector),\n",
    "                'sample_two_ohe_feat_manual needs to be a SparseVector')\n",
    "assert_true(isinstance(sample_three_ohe_feat_manual, SparseVector),\n",
    "                'sample_three_ohe_feat_manual needs to be a SparseVector')\n",
    "\n",
    "assert_equal(sample_one_ohe_feat_manual[2], 1.0, 'incorrect value for sample_one_ohe_feat_manual')\n",
    "assert_equal(sample_one_ohe_feat_manual[3], 1.0, 'incorrect value for sample_one_ohe_feat_manual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Define a OHE function\n",
    "\n",
    "Next we will use the OHE dictionary from Part (1a) to programatically generate OHE features from the original categorical data.  First write a function called `one_hot_encoding` that creates OHE feature vectors in `SparseVector` format.  Then use this function to create OHE features for the first sample data point and verify that the result matches the result from Part (1c).\n",
    "\n",
    "> Note: We'll pass in the OHE dictionary as a [Broadcast](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.Broadcast) variable, which will greatly improve performance when we call this function as part of a UDF. **When accessing a broadcast variable, you _must_ use `.value`.** For instance: `ohe_dict_broadcast.value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc626f1400b84737c828b47b66a67ad2",
     "grade": false,
     "grade_id": "answer_oheFunction1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    # <FILL IN>\n",
    "    # ohe_features = [<FILL IN>]\n",
    "    # return SparseVector(<FILL IN>)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "# # Calculate the number of features in sample_ohe_dict_manual\n",
    "# num_sample_ohe_feats = <FILL IN >\n",
    "# sample_ohe_dict_manual_broadcast = sc.broadcast(sample_ohe_dict_manual)\n",
    "\n",
    "# # Run one_hot_encoding() on sample_one.  Make sure to pass in the Broadcast variable.\n",
    "# sample_one_ohe_feat = <FILL IN >  \n",
    "  \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(sample_one_ohe_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cd84691fc8e6f2e773cb6e1da9d0772",
     "grade": true,
     "grade_id": "test_oheFunction1d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Define an OHE Function (1d)\n",
    "assert_true(sample_one_ohe_feat == sample_one_ohe_feat_manual,\n",
    "                'sample_one_ohe_feat should equal sample_one_ohe_feat_manual')\n",
    "assert_equal(sample_one_ohe_feat, SparseVector(7, [2, 3], [1.0, 1.0]),\n",
    "                  'incorrect value for sample_one_ohe_feat')\n",
    "assert_equal(one_hot_encoding([(1, 'black'), (0, 'mouse')], sample_ohe_dict_manual_broadcast,\n",
    "                                   num_sample_ohe_feats), SparseVector(7, [2, 3], [1.0, 1.0]),\n",
    "                  'incorrect definition for one_hot_encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) Apply OHE to a dataset\n",
    "\n",
    "Finally, use the function from Part (1d) to create OHE features for all 3 data points in the sample dataset.  You'll need to generate a [UDF](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.functions.udf) that can be used in a `DataFrame` `select` statement.\n",
    "\n",
    "> Note: Your implemenation of `ohe_udf_generator` needs to call your `one_hot_encoding` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49386eba93487b63c07d121c606c745a",
     "grade": false,
     "grade_id": "answer_oheUdfGeneratorFunction1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "def ohe_udf_generator(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "#     length = <FILL IN>\n",
    "#     return udf(lambda x: <FILL IN>, VectorUDT())\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# sample_ohe_dict_udf = ohe_udf_generator(sample_ohe_dict_manual_broadcast)\n",
    "# sample_ohe_df = sample_data_df.select( < FILL IN >)\n",
    "# sample_ohe_df.show(truncate=False)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe2dcbdb0d5e9ce5459b058ce61c7b20",
     "grade": true,
     "grade_id": "test_oheUdfGeneratorFunction1e",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Apply OHE to a dataset (1e)\n",
    "sample_ohe_data_values = sample_ohe_df.collect()\n",
    "assert_true(len(sample_ohe_data_values) == 3, 'sample_ohe_data_values should have three elements')\n",
    "assert_equal(sample_ohe_data_values[0], (SparseVector(7, {2: 1.0, 3: 1.0}),),\n",
    "                  'incorrect OHE for first sample')\n",
    "assert_equal(sample_ohe_data_values[1], (SparseVector(7, {1: 1.0, 4: 1.0, 5: 1.0}),),\n",
    "                  'incorrect OHE for second sample')\n",
    "assert_equal(sample_ohe_data_values[2], (SparseVector(7, {0: 1.0, 3: 1.0, 6: 1.0}),),\n",
    "                  'incorrect OHE for third sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Construct an OHE dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) DataFrame with rows of `(featureID, category)`\n",
    "\n",
    "To start, create a DataFrame of distinct `(feature_id, category)` tuples. In our sample dataset, the 7 items in the resulting DataFrame are `(0, 'bear')`, `(0, 'cat')`, `(0, 'mouse')`, `(1, 'black')`, `(1, 'tabby')`, `(2, 'mouse')`, `(2, 'salmon')`. Notably `'black'` appears twice in the dataset but only contributes one item to the DataFrame: `(1, 'black')`, while `'mouse'` also appears twice and contributes two items: `(0, 'mouse')` and `(2, 'mouse')`.  Use [explode](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) and [distinct](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9445adc1626d3bef10bed545a3d28ba4",
     "grade": false,
     "grade_id": "answer_dfWithRow2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import explode\n",
    "# sample_distinct_feats_df = (sample_data_df\n",
    "#                               <FILL IN>)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "sample_distinct_feats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98ef0325fd677f7c8fd669f44f04a479",
     "grade": true,
     "grade_id": "test_dfWithRow2a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST DataFrame with rows of `(featureID, category)` (2a)\n",
    "assert_equal(sorted(map(lambda r: r[0], sample_distinct_feats_df.collect())),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'incorrect value for sample_distinct_feats_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) OHE Dictionary from distinct features\n",
    "\n",
    "Next, create an RDD of key-value tuples, where each `(feature_id, category)` tuple in `sample_distinct_feats_df` is a key and the values are distinct integers ranging from 0 to (number of keys - 1).  Then convert this RDD into a dictionary, which can be done using the `collectAsMap` action.  Note that there is no unique mapping from keys to values, as all we require is that each `(featureID, category)` key be mapped to a unique integer between 0 and the number of keys.  In this exercise, any valid mapping is acceptable.  Use [zipWithIndex](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) followed by [collectAsMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap).\n",
    "\n",
    "In our sample dataset, one valid list of key-value tuples is: `[((0, 'bear'), 0), ((2, 'salmon'), 1), ((1, 'tabby'), 2), ((2, 'mouse'), 3), ((0, 'mouse'), 4), ((0, 'cat'), 5), ((1, 'black'), 6)]`. The dictionary defined in Part (1a) illustrates another valid mapping between keys and integers.\n",
    "\n",
    "> Note: We provide the code to convert the DataFrame to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca6524fe730cb4e6ebc20568f2f5a1d4",
     "grade": false,
     "grade_id": "answer_oheDict2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# sample_ohe_dict = (sample_distinct_feats_df\n",
    "#                      .rdd\n",
    "#                      .map(lambda r: tuple(r[0]))\n",
    "#                      <FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(sample_ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302b11cbdc6ad6285b359fc3bf82d5ae",
     "grade": true,
     "grade_id": "test_oheDict2b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST OHE Dictionary from distinct features (2b)\n",
    "assert_equal(sorted(sample_ohe_dict.keys()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'sample_ohe_dict has unexpected keys')\n",
    "assert_equal(sorted(sample_ohe_dict.values()), list(range(7)), 'sample_ohe_dict has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Automated creation of an OHE dictionary\n",
    "\n",
    "Now use the code from Parts (2a) and (2b) to write a function that takes an input dataset and outputs an OHE dictionary.  Then use this function to create an OHE dictionary for the sample dataset, and verify that it matches the dictionary from Part (2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e09487dcdeddfd148c58341b1bdf45e",
     "grade": false,
     "grade_id": "answer_createOneHotDictFunction2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def create_one_hot_dict(input_df):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n",
    "            (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "#    <FILL IN>\n",
    "#    distinct_feature_df = (<FILL IN>)\n",
    "#    key_value_tuple_list = (<FILL IN)\n",
    "#    return dict(key_value_tuple_list)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "sample_ohe_dict_auto = create_one_hot_dict(sample_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e6aef1078be6e30560f2e33fa5bc208",
     "grade": true,
     "grade_id": "test_createOneHotDictFunction2c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Automated creation of an OHE dictionary (2c)\n",
    "assert_equal(sorted(sample_ohe_dict_auto.keys()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'sample_ohe_dict_auto has unexpected keys')\n",
    "assert_equal(sorted(sample_ohe_dict_auto.values()), list(range(7)),\n",
    "                  'sample_ohe_dict_auto has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parse CTR data and generate OHE features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Before we can proceed, you'll first need to obtain the sample data.  ***\n",
    "\n",
    "The data is from a past [kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge/overview). The original data was too big. So we randomly samepled the data for this assignment from the original dataset.\n",
    "\n",
    "For data fileds:\n",
    "* Label - Target variable that indicates if an ad was clicked (1) or not (0).\n",
    "* I1-I13 - A total of 13 columns of integer features (mostly count features).\n",
    "* C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes.\n",
    "The semantic of the features is undisclosed.\n",
    "Just run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for getting data from the github repo.\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import Row\n",
    "url = \"https://raw.githubusercontent.com/10605/data/master/hw3/dac.txt\"\n",
    "sc.addFile(url)\n",
    "\n",
    "raw_df = sc.textFile(\"file://\" + SparkFiles.get(\"dac.txt\")).map(lambda r: Row(r)).toDF([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Loading and splitting the data\n",
    "\n",
    "We are now ready to start working with the actual CTR data, and our first task involves splitting it into training, validation, and test sets.  Usually we use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) with the specified weights and seed to create DFs storing each of these datasets. BUT randomSplit may generate non-deterministic results. So for the sake of testing, we manually split the data in to train, validation, test by the ratio of 0.8, 0.1, 0.1.\n",
    "\n",
    "Then your work is to [cache](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache) each of these DFs, as we will access them multiple times in the remainder of this lab. Finally, compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5edfe1a7841630a4bbdb3b75f15a82e3",
     "grade": false,
     "grade_id": "answer_loadSplitData3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "indexDf = raw_df.withColumn('index', f.monotonically_increasing_id())\n",
    "total_count = raw_df.count()\n",
    "train_count = int(total_count * 4 / 5)\n",
    "dev_count = int(total_count / 5)\n",
    "val_count = int(total_count / 10)\n",
    "test_count = int(total_count / 10)\n",
    "# first 20% rows\n",
    "raw_dev_df = indexDf.sort('index').limit(dev_count)\n",
    "# last 80% rows\n",
    "raw_train_df = indexDf.sort('index', ascending=False).limit(train_count).drop('index')\n",
    "# first 10% rows\n",
    "raw_validation_df = raw_dev_df.sort('index').limit(val_count).drop('index')\n",
    "# last 10% rows\n",
    "raw_test_df = raw_dev_df.sort('index', ascending=False).limit(test_count).drop('index')\n",
    "\n",
    "# # Cache and count the DataFrames\n",
    "# n_train = raw_train_df.<FILL IN>\n",
    "# n_val = raw_validation_df.<FILL IN>\n",
    "# n_test = raw_test_df.<FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(n_train, n_val, n_test, n_train + n_val + n_test)\n",
    "raw_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "889a702a433f661028204586b15e172e",
     "grade": true,
     "grade_id": "test_loadSplitData3a",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Loading and splitting the data (3a)\n",
    "assert_true(all([raw_train_df.is_cached, raw_validation_df.is_cached, raw_test_df.is_cached]),\n",
    "                'you must cache the split data')\n",
    "assert_equal(n_train, 80000, 'incorrect value for n_train')\n",
    "assert_equal(n_val, 10000, 'incorrect value for n_val')\n",
    "assert_equal(n_test, 10000, 'incorrect value for n_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Extract features\n",
    "\n",
    "We will now parse the raw training data in order to create a DataFrame that we can subsequently use to create an OHE dictionary. Note from the `show()` command in Part (3a) that each raw data point is a string containing several fields separated by some delimiters.  For now, we will ignore the first field (which is just the 0-1 label), and parse the remaining fields (or raw features).  To do this, complete the implemention of the `parse_point` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eec736bbcaddca09efa3b229451ed55d",
     "grade": false,
     "grade_id": "answer_extractFeatures3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def parse_point(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "#       <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(parse_point(raw_df.select('text').first()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d38c6262d91db01488cec1b521db53a",
     "grade": true,
     "grade_id": "test_extractFeatures3b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Extract features (3b)\n",
    "assert_equal(parse_point(raw_df.select('text').first()[0])[:3], [(0, u'0'), (1, u'127'), (2, u'1')],\n",
    "                  'incorrect implementation of parse_point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Extracting features continued\n",
    "\n",
    "Next, we'll create a `parse_raw_df` function that creates a `label` column for the first value in a data point and a `features` column for the rest.  The `features` column will be created using `parse_point_udf`, which we've provided and is based on your `parse_point` function.  Note that to name your columns you should use [alias](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.alias).  You can split the `text` field in `raw_df` using [split](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split) and retrieve the first value of the resulting array with [getItem](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.getItem). Be sure to call [cast](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast) to cast the column value to `double`. Your `parse_raw_df` function should also cache the DataFrame it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "271458166fa96d2d7e85830296be31ea",
     "grade": false,
     "grade_id": "answer_extractFreatures3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with the appropriate code\n",
    "from pyspark.sql.functions import udf, split\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType\n",
    "\n",
    "parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),\n",
    "                                                         StructField('_2', StringType())])))\n",
    "\n",
    "def parse_raw_df(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and features.\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'features' columns.\n",
    "    \"\"\"\n",
    "#     return raw_df.select(<FILL IN>)\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .<FILL IN>\n",
    "#           .cache()\n",
    "\n",
    "  \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Parse the raw training DataFrame\n",
    "# parsed_train_df = <FILL IN>  \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "from pyspark.sql.functions import (explode, col)\n",
    "num_categories = (parsed_train_df\n",
    "                   .select(explode('features').alias('features'))\n",
    "                   .distinct()\n",
    "                   .select(col('features').getField('_1').alias('featureNumber'))\n",
    "                   .groupBy('featureNumber')\n",
    "                   .sum()\n",
    "                   .orderBy('featureNumber')\n",
    "                   .collect())\n",
    "\n",
    "print(num_categories[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "966f2aff815951fc7239a33735eea753",
     "grade": true,
     "grade_id": "test_extractFreatures3c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Extract features (3c)\n",
    "assert_true(parsed_train_df.is_cached, 'parse_raw_df should return a cached DataFrame')\n",
    "assert_equal(num_categories[2][1], 1356, 'incorrect implementation of parse_point or parse_raw_df')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Create an OHE dictionary from the dataset\n",
    "\n",
    "Note that `parse_point` returns a data point in the format of a list of `(featureID, category)` tuples, which is the same format as the sample dataset studied in Parts 1 and 2 of this lab.  Using this observation, create an OHE dictionary from the parsed training data using the function implemented in Part (2c). Note that we will assume for simplicity that all features in our CTR dataset are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "febc19848d31727c9ce0a8e0b7da729b",
     "grade": false,
     "grade_id": "answer_createOheDict3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# ctr_ohe_dict = <FILL IN>\n",
    "# num_ctr_ohe_feats = <FILL IN>\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(ctr_ohe_dict[(0, '4')])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3129635b82361eb5c8179ce0db401a4b",
     "grade": true,
     "grade_id": "test_createOheDict3d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Create an OHE dictionary from the dataset (3d)\n",
    "assert_equal(num_ctr_ohe_feats, 215556, 'incorrect number of features in ctr_ohe_dict')\n",
    "assert_true((0, '4') in ctr_ohe_dict, 'incorrect features in ctr_ohe_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Apply OHE to the dataset\n",
    "\n",
    "Now let's use this OHE dictionary, by starting with the training data that we've parsed into `label` and `features` columns, to create one-hot-encoded features.  Recall that we created a function `ohe_udf_generator` that can create the UDF that we need to convert row into `features`.  Make sure that `ohe_train_df` contains a `label` and `features` column and is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e91545c6e2840a0ca85242cef82079b",
     "grade": false,
     "grade_id": "answer_applyOhe3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with the appropriate code\n",
    "# ohe_dict_broadcast = <FILL IN>\n",
    "# ohe_dict_udf = <FILL IN>\n",
    "# ohe_train_df = (parsed_train_df\n",
    "#                   <FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(ohe_train_df.count())\n",
    "print(ohe_train_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4681747ea5811311803a275f89205723",
     "grade": true,
     "grade_id": "test_applyOhe3e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Apply OHE to the dataset (3e)\n",
    "assert_true('label' in ohe_train_df.columns and 'features' in ohe_train_df.columns, 'ohe_train_df should have label and features columns')\n",
    "assert_true(ohe_train_df.is_cached, 'ohe_train_df should be cached')\n",
    "num_nz = sum(parsed_train_df.rdd.map(lambda r: len(r[1])).take(5))\n",
    "num_nz_alt = sum(ohe_train_df.rdd.map(lambda r: len(r[1].indices)).take(5))\n",
    "assert_equal(num_nz, num_nz_alt, 'incorrect value for ohe_train_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Feature frequency\n",
    "\n",
    "We will now visualize the number of times each of the 233,941 OHE features appears in the training data. We first compute the number of times each feature appears, then bucket the features by these counts.  The buckets are sized by powers of 2, so the first bucket corresponds to features that appear exactly once ( \\\\( \\scriptsize 2^0 \\\\) ), the second to features that appear twice ( \\\\( \\scriptsize 2^1 \\\\) ), the third to features that occur between three and four ( \\\\( \\scriptsize 2^2 \\\\) ) times, the fifth bucket is five to eight ( \\\\( \\scriptsize 2^3 \\\\) ) times and so on. The scatter plot below shows the logarithm of the bucket thresholds versus the logarithm of the number of features that have counts that fall in the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "get_indices = udf(lambda sv: list(map(int, sv.indices)), ArrayType(IntegerType()))\n",
    "feature_counts = (ohe_train_df\n",
    "                   .select(explode(get_indices('features')))\n",
    "                   .groupBy('col')\n",
    "                   .count()\n",
    "                   .withColumn('bucket', log('count').cast('int'))\n",
    "                   .groupBy('bucket')\n",
    "                   .count()\n",
    "                   .orderBy('bucket')\n",
    "                   .collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = zip(*feature_counts)\n",
    "x, y = x, np.log(y)\n",
    "\n",
    "def prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n",
    "                 grid_width=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hide_labels: axis.set_ticklabels([])\n",
    "    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 12, 1), np.arange(0, 14, 2))\n",
    "ax.set_xlabel(r'$\\log_e(bucketSize)$'), ax.set_ylabel(r'$\\log_e(countInBucket)$')\n",
    "plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Handling unseen features\n",
    "\n",
    "We naturally would like to repeat the process from Part (3e), to compute OHE features for the validation and test datasets.  However, we must be careful, as some categorical values will likely appear in new data that did not exist in the training data. To deal with this situation, update the `one_hot_encoding()` function from Part (1d) to ignore previously unseen categories, and then compute OHE features for the validation data.  Remember that you can parse a raw DataFrame using `parse_raw_df`.\n",
    "> Note: you'll have to generate a new UDF using `ohe_udf_generator` so that the updated `one_hot_encoding` function is used.  And make sure to cache `ohe_validation_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "716994760bdd6599db881a44635f2c48",
     "grade": false,
     "grade_id": "answer_handelUnseenFeatures",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted, and that the\n",
    "        function handles missing features.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "#     ohe_feature = <FILL IN>\n",
    "#     return SparseVector(<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# ohe_dict_missing_udf = <FILL IN>\n",
    "# ohe_validation_df = (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "ohe_validation_df.count()\n",
    "display(ohe_validation_df) # replace with ohe_validate_df.show() if running outside of Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abb36187be8db563de629175be61bf1f",
     "grade": true,
     "grade_id": "test_handelUnseenFeatures",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Handling unseen features (3f)\n",
    "from pyspark.sql.functions import size, sum as sqlsum\n",
    "\n",
    "assert_true(ohe_validation_df.is_cached, 'you need to cache ohe_validation_df')\n",
    "num_nz_val = (ohe_validation_df\n",
    "                .select(sqlsum(size(get_indices('features'))))\n",
    "                .first()[0])\n",
    "\n",
    "nz_expected = 367573\n",
    "assert_equal(num_nz_val, nz_expected, 'incorrect number of features: Got {0}, expected {1}'.format(num_nz_val, nz_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CTR prediction and logloss evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Logistic regression\n",
    "\n",
    "We are now ready to train our first CTR classifier.  A natural classifier for this setting is logistic regression, since it models the probability of a click-through event rather than returning a simple binary \"yes\" or \"no\". Also, when working with rare events like clicking-through, probabilistic predictions are usually more accurate.\n",
    "\n",
    "First use [LogisticRegression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) from the pyspark.ml package to train a model using `ohe_train_df` with a given hyperparameter configuration.  `LogisticRegression.fit` returns a [LogisticRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegressionModel).  Next, we'll use the `LogisticRegressionModel.coefficients` and `LogisticRegressionModel.intercept` to print out some details of the model's parameters.  Note that these are the names of the object's attributes and should be called using a syntax like `model.coefficients` for a given `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76a0dc800897f1d1dbccc40a33cdc001",
     "grade": false,
     "grade_id": "answer_logisticReg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# Given hyperparameters\n",
    "standardization = False\n",
    "elastic_net_param = 0.0\n",
    "reg_param = .01\n",
    "max_iter = 20\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# lr = (<FILL IN>)\n",
    "\n",
    "# lr_model_basic = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_basic.intercept))\n",
    "print('length of coefficients: {0}'.format(len(lr_model_basic.coefficients)))\n",
    "sorted_coefficients = sorted(lr_model_basic.coefficients)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa5fee2e7d28fb46ee61879453be78a0",
     "grade": true,
     "grade_id": "test_logisticReg",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Logistic regression (4a)\n",
    "assert_true(np.allclose(lr_model_basic.intercept,  -1.1870497039599432, atol=1e-2), 'incorrect value for model intercept')\n",
    "assert_true(np.allclose(sorted_coefficients,\n",
    "                           [-0.10347285277044568, -0.10296978958368273, -0.10296978958368273, -0.10296978958368273, -0.10296978958368273], atol=1e-2),\n",
    "                           'incorrect value for model coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Log loss\n",
    "Throughout this lab, we will use log loss to evaluate the quality of models.  Log loss is defined as: \\\\[ \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\\\] where \\\\( \\scriptsize p\\\\) is a probability between 0 and 1 and \\\\( \\scriptsize y\\\\) is a label of either 0 or 1. Log loss is a standard evaluation criterion when predicting rare-events such as click-through rate prediction.\n",
    "\n",
    "Write a function `add_log_loss` for a DataFrame, and evaluate it on sample inputs.  This operation does not require a UDF.  You can perform a conditional branching with DataFrame columns using [when](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.when)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example data\n",
    "example_log_loss_df = sqlContext.createDataFrame([(.5, 1), (.5, 0), (.99, 1), (.99, 0), (.01, 1),\n",
    "                                                  (.01, 0), (1., 1), (.0, 1), (1., 0)], ['p', 'label'])\n",
    "example_log_loss_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "468ac9f43fbc40fb2fe48bafcc9d25ab",
     "grade": false,
     "grade_id": "answer_logLoss4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import when, log, col\n",
    "epsilon = 1e-16\n",
    "\n",
    "def add_log_loss(df):\n",
    "    \"\"\"Computes and adds a 'log_loss' column to a DataFrame using 'p' and 'label' columns.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we add a small value (epsilon) to it and when\n",
    "        p is 1 we subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'p' and 'label' columns): A DataFrame with a probability column\n",
    "            'p' and a 'label' column that corresponds to y in the log loss formula.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with an additional column called 'log_loss'\n",
    "    \"\"\"\n",
    "#     return df.withColum(<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "add_log_loss(example_log_loss_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "874f346d650700606ab6ea71b57512fe",
     "grade": true,
     "grade_id": "test_logLoss4b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Log loss (4b)\n",
    "log_loss_values = add_log_loss(example_log_loss_df).select('log_loss').rdd.map(lambda r: r[0]).collect()\n",
    "assert_true(np.allclose(log_loss_values[:-2],\n",
    "                            [0.6931471805599451, 0.6931471805599451, 0.010050335853501338, 4.60517018598808,\n",
    "                             4.605170185988081, 0.010050335853501338, -0.0], atol=1e-2), 'log loss is not correct')\n",
    "assert_true(not(any(map(lambda x: x is None, log_loss_values[-2:]))),\n",
    "                'log loss needs to bound p away from 0 and 1 by epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c)  Baseline log loss\n",
    "\n",
    "Next we will use the function we have in Part (4b) to compute the baseline log loss of the training data. A very simple yet natural baseline model is that we always make the same prediction regardless of datapoints, therefore the predicted value would be equal to the fraction of training points that correspond to click-through events (i.e., where the label is one). Compute this value (which is simply the mean of the training labels), and then use it to compute the training log loss for the baseline model.\n",
    "\n",
    "> Note: you'll need to add a `p` column to the `ohe_train_df` DataFrame so that it can be used in your function from Part (4b).  To represent a constant value as a column you can use the [lit](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lit) function to wrap the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2dc5175b1299ca363fef7c52b831f15",
     "grade": false,
     "grade_id": "answer_baselineLogLoss4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# Note that our dataset has a very high click-through rate by design\n",
    "# In practice click-through rate can be one to two orders of magnitude lower\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# class_one_frac_train = (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print('Training class one fraction = {0:.3f}'.format(class_one_frac_train))\n",
    "\n",
    "# log_loss_tr_base = (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print('Baseline Train Logloss = {0:.3f}\\n'.format(log_loss_tr_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "baf209e488a24cfdf2fd4903ceb2d0ed",
     "grade": true,
     "grade_id": "test_baselineLogLoss4c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Baseline log loss (4c)\n",
    "expected_frac = 0.2339125\n",
    "expected_log_loss = 0.5439608117656105\n",
    "assert_true(np.allclose(class_one_frac_train, expected_frac, atol=1e-2), 'incorrect value for class_one_frac_train. Got {0}, expected {1}'.format(class_one_frac_train, expected_frac))\n",
    "assert_true(np.allclose(log_loss_tr_base, expected_log_loss, atol=1e-2), 'incorrect value for log_loss_tr_base. Got {0}, expected {1}'.format(log_loss_tr_base, expected_log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Predict probability\n",
    "\n",
    "In order to compute the log loss for the model we trained in Part (4a), we need to generate predictions from this model. Write a function that computes the raw linear prediction from this logistic regression model and then passes it through a [sigmoid function](http://en.wikipedia.org/wiki/Sigmoid_function) \\\\( \\scriptsize \\sigma(t) = (1+ e^{-t})^{-1} \\\\) to return the model's probabilistic prediction. Then compute probabilistic predictions on the training data.\n",
    "\n",
    "Note that when incorporating an intercept into our predictions, we simply add the intercept to the value of the prediction obtained from the weights and features.  Alternatively, if the intercept was included as the first weight, we would need to add a corresponding feature to our data where the feature has the value one.  This is not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db04f85c1d270a21737aa738642a0341",
     "grade": false,
     "grade_id": "answer_predictProb4d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def add_probability(df, model):\n",
    "    \"\"\"Adds a probability column ('p') to a DataFrame given a model\"\"\"\n",
    "    coefficients_broadcast = sc.broadcast(model.coefficients)\n",
    "    intercept = model.intercept\n",
    "\n",
    "    def get_p(features):\n",
    "        \"\"\"Calculate the probability for an observation given a list of features.\n",
    "\n",
    "        Note:\n",
    "            We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "        Args:\n",
    "            features: the features\n",
    "\n",
    "        Returns:\n",
    "            float: A probability between 0 and 1.\n",
    "        \"\"\"\n",
    "#         # Compute the raw value\n",
    "#         raw_prediction = <FILL IN>\n",
    "#         # Bound the raw value between 20 and -20\n",
    "#         raw_prediction = <FILL IN>\n",
    "#         # Return the probability\n",
    "#         <FILL IN>\n",
    "        \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "    get_p_udf = udf(get_p, DoubleType())\n",
    "    return df.withColumn('p', get_p_udf('features'))\n",
    "\n",
    "add_probability_model_basic = lambda df: add_probability(df, lr_model_basic)\n",
    "training_predictions = add_probability_model_basic(ohe_train_df).cache()\n",
    "\n",
    "training_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "655f867ffa29d7f76d496792238ee7ca",
     "grade": true,
     "grade_id": "test_predictProb4d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Predicted probability (4d)\n",
    "expected = 18746.356946150863\n",
    "got = training_predictions.selectExpr('sum(p)').first()[0]\n",
    "assert_true(np.allclose(got, expected, atol=1e-2),\n",
    "                'incorrect value for training_predictions. Got {0}, expected {1}'.format(got, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Evaluate the model\n",
    "\n",
    "We are now ready to evaluate the performance of the model we trained in Part (4a). To do this, first write a general function that takes a model and a DataFrame as its input, and outputs the log loss. Note that the log loss for multiple observations should be the mean of all the individual log losses. Then run this function on the OHE training data, and compare the result with the baseline log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f89174135486c5a70cc24d30bee3eb0b",
     "grade": false,
     "grade_id": "answer_evaluateModel4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def evaluate_results(df, model, baseline=None):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Note:\n",
    "        If baseline has a value the probability should be set to baseline before\n",
    "        the log loss is calculated.  Otherwise, use add_probability to add the\n",
    "        appropriate probabilities to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'label' and 'features' columns): A DataFrame containing\n",
    "            labels and features.\n",
    "        model (LogisticRegressionModel): A trained logistic regression model. This\n",
    "            can be None if baseline is set.\n",
    "        baseline (float): A baseline probability to use for the log loss calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    \n",
    "#     with_probability_df = <FILL IN>\n",
    "#     with_log_loss_df = <FILL IN>\n",
    "#     log_loss = <FILL IN>\n",
    "#     return log_loss\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "log_loss_train_model_basic = evaluate_results(ohe_train_df, lr_model_basic)\n",
    "print('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8ee8a01b000b8df5dc15b49032bcd53",
     "grade": true,
     "grade_id": "test_evaluateModel4e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Evaluate the model (4e)\n",
    "expected_log_loss = 0.48971226940239815\n",
    "assert_true(np.allclose(log_loss_train_model_basic, expected_log_loss, atol=1e-2),\n",
    "                'incorrect value for log_loss_train_model_basic. Got {0}, expected {1}'.format(log_loss_train_model_basic, expected_log_loss))\n",
    "expected_res = 0.6931471805600546\n",
    "res = evaluate_results(ohe_train_df, None,  0.5)\n",
    "assert_true(np.allclose(res, expected_res, atol=1e-2),\n",
    "                'evaluate_results needs to handle baseline models. Got {0}, expected {1}'.format(res, expected_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Log loss of validation dataset\n",
    "\n",
    "Next, use the `evaluate_results` function and compute the log loss of validation dataset for both the baseline and logistic regression models. Notably, the baseline model for the validation dataset should still be based on the label fraction from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae5f2e641ede01f12b0c8f596601578b",
     "grade": false,
     "grade_id": "answer_valicationLogLossf4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# log_loss_val_base = <FILL IN>\n",
    "\n",
    "# log_loss_val_l_r0 = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_val_base, log_loss_val_l_r0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50502f77e9ba4cb9f6c3286536e44667",
     "grade": true,
     "grade_id": "test_valicationLogLossf4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Validation log loss (4f)\n",
    "expected_val_base = 0.6344644324013423\n",
    "assert_true(np.allclose(log_loss_val_base, expected_val_base, atol=1e-2),\n",
    "                'incorrect value for log_loss_val_base. Got {0}, expected {1}'.format(log_loss_val_base, expected_val_base))\n",
    "expected_val_model_basic = 0.5793520014798194\n",
    "assert_true(np.allclose(log_loss_val_l_r0, expected_val_model_basic, atol=1e-2),\n",
    "                'incorrect value for log_loss_val_l_r0. Got {0}, expected {1}'.format(log_loss_val_l_r0, expected_val_model_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: ROC curve\n",
    "\n",
    "We will now visualize the performance of our model.  We generate a plot called the ROC curve.  The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalizing the threshold required for a positive prediction.  The performance of a random model is represented by the dashed line in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_scores = add_probability_model_basic(ohe_validation_df).select('label', 'p')\n",
    "labels_and_weights = labels_and_scores.collect()\n",
    "labels_and_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "labels_by_weight = np.array([k for (k, v) in labels_and_weights])\n",
    "\n",
    "length = labels_by_weight.size\n",
    "true_positives = labels_by_weight.cumsum()\n",
    "num_positive = true_positives[-1]\n",
    "false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
    "\n",
    "true_positive_rate = true_positives / num_positive\n",
    "false_positive_rate = false_positives / (length - num_positive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Reduce features' dimension via feature hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5a) Hash function\n",
    "\n",
    "As we just saw, using an one-hot-encoding featurization can yield a model with good statistical accuracy.  However, the number of distinct categories across all features is quite large -- recall that we observed 233K categories in the training data in Part (3c).  Moreover, the full training dataset includes more than 33M distinct categories, and the training dataset itself is just a small subset of labeled data in real world.  Hence, featurizing via an one-hot-encoding representation could lead to a very large feature vector. To reduce the dimensionality of the feature space, we will use feature hashing.\n",
    "\n",
    "Below is a hash function that we will use for this part of the lab.  We will first use this hash function with the three sample data points from Part (1a) to gain some intuition.  Implement the following code to hash those three sample points using two different values for `numBuckets` and observe the resulting hashed feature dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import functools \n",
    "import hashlib\n",
    "\n",
    "def hash_function(raw_feats, num_buckets, print_mapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        num_buckets (int): Number of buckets to use as features.\n",
    "        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = { category + ':' + str(ind):\n",
    "                int(int(hashlib.md5((category + ':' + str(ind)).encode('utf-8')).hexdigest(), 16) % num_buckets)\n",
    "                for ind, category in raw_feats}\n",
    "    if(print_mapping): print(mapping)\n",
    "\n",
    "    def map_update(l, r):\n",
    "        l[r] += 1.0\n",
    "        return l\n",
    "\n",
    "    sparse_features = functools.reduce(map_update, mapping.values(), defaultdict(float))\n",
    "    return dict(sparse_features)\n",
    "\n",
    "# Reminder of the sample values:\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8766444e4b11b332433d342420e59f6c",
     "grade": false,
     "grade_id": "answer_hashFunction5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# # Use four buckets\n",
    "# samp_one_four_buckets = hash_function(sample_one, < FILL IN >, True)\n",
    "# samp_two_four_buckets = hash_function(sample_two, < FILL IN >, True)\n",
    "# samp_three_four_buckets = hash_function(sample_three, < FILL IN >, True)\n",
    "\n",
    "# # Use one hundred buckets\n",
    "# samp_one_hundred_buckets = hash_function(sample_one, < FILL IN >, True)\n",
    "# samp_two_hundred_buckets = hash_function(sample_two, < FILL IN >, True)\n",
    "# samp_three_hundred_buckets = hash_function(sample_three, < FILL IN >, True)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('\\n\\t\\t 4 Buckets \\t\\t\\t 100 Buckets')\n",
    "print('Sample One:\\t {0}\\t\\t\\t {1}'.format(samp_one_four_buckets, samp_one_hundred_buckets))\n",
    "print('Sample Two:\\t {0}\\t\\t {1}'.format(samp_two_four_buckets, samp_two_hundred_buckets))\n",
    "print('Sample Three:\\t {0}\\t {1}'.format(samp_three_four_buckets, samp_three_hundred_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3066187788b5f62a8f6ff9c566cf6fe3",
     "grade": true,
     "grade_id": "test_hashFunction5a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Hash function (5a)\n",
    "assert_equal(samp_one_four_buckets, {3: 2.0}, 'incorrect value for samp_one_four_buckets')\n",
    "assert_equal(samp_three_hundred_buckets, {80: 1.0, 82: 1.0, 51: 1.0},\n",
    "                  'incorrect value for samp_three_hundred_buckets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5b) Create hashed features\n",
    "\n",
    "Next, we will use this hash function to create hashed features for our CTR datasets. Use the given UDF to create a function that takes in a DataFrame and returns both labels and hashed features.  Then use it to create new training, validation and test datasets with hashed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa9920b4a741c7bf013eb6d054ab25f",
     "grade": false,
     "grade_id": "answer_createHashFeatures5b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "from pyspark.ml.linalg import Vectors\n",
    "num_hash_buckets = 2 ** 15\n",
    "\n",
    "# UDF that returns a vector of hashed features given an Array of tuples\n",
    "tuples_to_hash_features_udf = udf(lambda x: Vectors.sparse(num_hash_buckets, hash_function(x, num_hash_buckets)), VectorUDT())\n",
    "\n",
    "def add_hashed_features(df):\n",
    "    \"\"\"Return a DataFrame with labels and hashed features.\n",
    "\n",
    "    Note:\n",
    "        Make sure you cache the DataFrame that you are returning.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'label' and 'features' column): A DataFrame containing labels and the features to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n",
    "            SparseVector of hashed features.\n",
    "    \"\"\"\n",
    "#     return     <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# hash_train_df = <FILL IN>\n",
    "# hash_validation_df = <FILL IN>\n",
    "# hash_test_df = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "hash_train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c40949b19e413b0afe05bb6344587b3",
     "grade": true,
     "grade_id": "test_createHashFeatures5b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Creating hashed features (5b)\n",
    "hash_train_df_feature_sum = sum(hash_train_df\n",
    "                                  .rdd\n",
    "                                  .map(lambda r: sum(r[1].indices))\n",
    "                                  .take(10))\n",
    "hash_validation_df_feature_sum = sum(hash_validation_df\n",
    "                                       .rdd\n",
    "                                       .map(lambda r: sum(r[1].indices))\n",
    "                                       .take(10))\n",
    "hash_test_df_feature_sum = sum(hash_test_df\n",
    "                                 .rdd\n",
    "                                 .map(lambda r: sum(r[1].indices))\n",
    "                                 .take(10))\n",
    "\n",
    "expected_train_sum = 6333443\n",
    "assert_equal(hash_train_df_feature_sum, expected_train_sum,\n",
    "                  'incorrect number of features in hash_train_df. Got {0}, expected {1}'.format(hash_train_df_feature_sum, expected_train_sum))\n",
    "\n",
    "expected_validation_sum = 6340030\n",
    "assert_equal(hash_validation_df_feature_sum, expected_validation_sum,\n",
    "                  'incorrect number of features in hash_validation_df. Got {0}, expected {1}'.format(hash_validation_df_feature_sum, expected_validation_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5c) Sparsity\n",
    "\n",
    "Since we only have 33K hashed features versus 233K OHE features, we could expect our OHE features to be sparser. Verify this hypothesis by computing the average sparsity of the OHE and the hashed training datasets.\n",
    "\n",
    "Note that if you have a `SparseVector` named `sparse`, calling `len(sparse)` returns the total number of features, not the number features with entries.  `SparseVector` objects have the attributes `indices` and `values` that contain information about non-zero features.  Continuing with our example, these can be accessed using `sparse.indices` and `sparse.values`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e37c6bfd27cb45276768bf5e34bcf734",
     "grade": false,
     "grade_id": "answer_sparsity5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "def vector_feature_sparsity(sparse_vector):\n",
    "    \"\"\"Calculates the sparsity of a SparseVector.\n",
    "\n",
    "    Args:\n",
    "        sparse_vector (SparseVector): The vector containing the features.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of features found in the vector to the total number of features.\n",
    "    \"\"\"\n",
    "#     return     <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "a_sparse_vector = Vectors.sparse(5, {0: 1.0, 3: 1.0})\n",
    "a_sparse_vector_sparsity = vector_feature_sparsity(a_sparse_vector)\n",
    "print('This vector should have sparsity 2/5 or .4.')\n",
    "print('Sparsity = {0:.2f}.'.format(a_sparse_vector_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6ca86abd9beaf7b1e0456ec9b863b47",
     "grade": true,
     "grade_id": "test_sparsity5c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sparsity (5c)\n",
    "assert_equal(a_sparse_vector_sparsity, .4,\n",
    "                'incorrect value for a_sparse_vector_sparsity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5d) Sparsity continued\n",
    "\n",
    "Now we have a function to calculate vector sparsity, we'll wrap it up in a UDF and apply it to the entire DataFrame to obtain the average sparsity of features in that DataFrame.  We'll use this function to calculate the average sparsity of both the one-hot-encoded training DataFrame and  the hashed training DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "046048a2b4ed5b35f55c1f78623d1da2",
     "grade": false,
     "grade_id": "answer_sparsity5d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "feature_sparsity_udf = udf(vector_feature_sparsity, DoubleType())\n",
    "\n",
    "def get_sparsity(df):\n",
    "    \"\"\"Calculates the average sparsity for the features in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'features' column): A DataFrame with sparse features.\n",
    "\n",
    "    Returns:\n",
    "        float: The average feature sparsity.\n",
    "    \"\"\"\n",
    "#     return (<FILL IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# average_sparsity_ohe = <FILL IN>\n",
    "# average_sparsity_hash = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Average OHE Sparsity: {0:.7e}'.format(average_sparsity_ohe))\n",
    "print('Average Hash Sparsity: {0:.7e}'.format(average_sparsity_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44b1e0512bb3ed807ee64aaf3bebc950",
     "grade": true,
     "grade_id": "test_sparsity5d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sparsity (5d)\n",
    "expected_ohe = 1.8092746e-04\n",
    "assert_true(np.allclose(average_sparsity_ohe, expected_ohe, atol=1e-2),\n",
    "                'incorrect value for average_sparsity_ohe. Got {0}, expected {1}'.format(average_sparsity_ohe, expected_ohe))\n",
    "expected_hash = 1.1895943e-03\n",
    "assert_true(np.allclose(average_sparsity_hash, expected_hash, atol=1e-2),\n",
    "                'incorrect value for average_sparsity_hash. Got {0}, expected {1}'.format(average_sparsity_hash, expected_hash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5e) Logistic model with hashed features\n",
    "\n",
    "Now let's train a logistic regression model using the hashed training features. Use the given hyperparameters, train and fit the model, then evaluate the log loss on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef61ef1ac18a7f3e504f51741403bb12",
     "grade": false,
     "grade_id": "answer_modelWithHashFeature5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# Given hyperparameters\n",
    "standardization = False\n",
    "elastic_net_param = 0.7\n",
    "reg_param = .001\n",
    "max_iter = 20\n",
    "\n",
    "# lr_hash = (<FILL IN>)\n",
    "\n",
    "# lr_model_hashed = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_hashed.intercept))\n",
    "print(len(lr_model_hashed.coefficients))\n",
    "\n",
    "log_loss_train_model_hashed = evaluate_results(hash_train_df, lr_model_hashed)\n",
    "print(('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\thashed = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_hashed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdae0b218a20d5acd5842141dc17b06b",
     "grade": true,
     "grade_id": "test_modelWithHashFeature5e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Logistic model with hashed features (5e)\n",
    "expected =  0.481478172974873\n",
    "assert_true(np.allclose(log_loss_train_model_hashed, expected, atol=1e-2),\n",
    "                'incorrect value for log_loss_train_model_hashed. Got {0}, expected {1}'.format(log_loss_train_model_hashed, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5f) Evaluate the performance on the test set\n",
    "\n",
    "Finally, we will evaluate the model from Part (5e) on the test set.  Compare the resulting log loss with the baseline log loss on the test set, which can be computed in the same way where the validation log loss was computed in Part (4f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8824f00fa7475804eaa2b607e4e30f62",
     "grade": false,
     "grade_id": "answer_evaluateOnTestset5f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
    "# # Log loss for the best model from (5e)\n",
    "# log_loss_test = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# ## Log loss for the baseline model\n",
    "# class_one_frac_test = <FILL IN>\n",
    "# print('Class one fraction for test data: {0}'.format(class_one_frac_test))\n",
    "# log_loss_test_baseline = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(('Hashed Features Test Log Loss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_test_baseline, log_loss_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2181a1e0646dc9acbe4c171311ba9a7",
     "grade": true,
     "grade_id": "test_evaluateOnTestset5f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Evaluate on the test set (5f)\n",
    "expected_test_baseline = 0.6257336255412367\n",
    "assert_true(np.allclose(log_loss_test_baseline, expected_test_baseline, atol=1e-2),\n",
    "                'incorrect value for log_loss_test_baseline. Got {0}, expected {1}'.format(log_loss_test_baseline, expected_test_baseline))\n",
    "expected_test = 0.5748571343610652\n",
    "assert_true(np.allclose(log_loss_test, expected_test, atol=1e-2),\n",
    "                'incorrect value for log_loss_test. Got {0}, expected {1}'.format(log_loss_test, expected_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "In this part we will dive into exploratory analysis of neuroscience data, specifically using principal component analysis (PCA) and feature-based aggregation. We will use a dataset of light-sheet imaging recorded by the [Ahrens Lab](http://www.janelia.org/lab/ahrens-lab) at Janelia Research Campus.\n",
    "\n",
    "Our dataset is generated by studying the movement of a larval [zebrafish](http://en.wikipedia.org/wiki/Zebrafish), an animal that is especially useful in neuroscience because it is transparent, making it possible to record activity over its entire brain using a technique called [light-sheet microscopy](http://en.wikipedia.org/wiki/Light_sheet_fluorescence_microscopy).   Specifically, we'll work with time-varying images containing patterns of the zebrafish's neural activity as it is presented with a moving visual pattern.   Different stimuli induce different patterns across the brain, and we can use exploratory analyses to identify these patterns.  Read [\"Mapping brain activity at scale with cluster computing\"](http://thefreemanlab.com/work/papers/freeman-2014-nature-methods.pdf) for more information about these kinds of analyses.\n",
    "\n",
    "During this lab you will learn about PCA, and then compare and contrast different exploratory analyses of the same data set to identify which neural patterns they best highlight.\n",
    "\n",
    "## This section will cover:\n",
    "\n",
    "*  *Part 1:* Work through the steps of PCA on a sample dataset\n",
    " * *Visualization 1:* Two-dimensional Gaussians\n",
    "\n",
    "*  *Part 2:* Write a PCA function and evaluate PCA on sample datasets\n",
    " * *Visualization 2:* PCA projection\n",
    " * *Visualization 3:* Three-dimensional data\n",
    " * *Visualization 4:* 2D representation of 3D data\n",
    "\n",
    "*  *Part 3:* Parse, inspect, and preprocess neuroscience data then perform PCA\n",
    " * *Visualization 5:* Pixel intensity\n",
    " * *Visualization 6:* Normalized data\n",
    " * *Visualization 7:* Top two components as images\n",
    " * *Visualization 8:* Top two components as one image\n",
    "\n",
    "*  *Part 4:* Perform feature-based aggregation followed by PCA\n",
    " * *Visualization 9:* Top two components by time\n",
    " * *Visualization 10:* Top two components by direction\n",
    "\n",
    "> Note that, for reference, you can look up the details of:\n",
    "> * the relevant Spark methods in [PySpark's DataFrame API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.DataFrame)\n",
    "> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Work through the steps of PCA on a sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Two-dimensional Gaussians\n",
    "\n",
    "Principal Component Analysis, or PCA, is a strategy for dimensionality reduction. To better understand PCA, we'll work with synthetic data generated by sampling from the [two-dimensional Gaussian distribution](http://en.wikipedia.org/wiki/Multivariate_normal_distribution).  This distribution takes as input the mean and variance of each dimension, as well as the covariance between the two dimensions.\n",
    "\n",
    "In our visualizations below, we will specify the mean of each dimension to be 50 and the variance along each dimension to be 1.  We will explore two different values for the covariance: 0 and 0.9. When the covariance is zero, the two dimensions are uncorrelated, and hence the data looks spherical.  In contrast, when the covariance is 0.9, the two dimensions are strongly (positively) correlated and thus the data is non-spherical.  As we'll see in Parts 1 and 2, the non-spherical data is amenable to dimensionality reduction via PCA, while the spherical data is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n",
    "                 grid_width=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hide_labels: axis.set_ticklabels([])\n",
    "    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "def create_2D_gaussian(mn, variance, cov, n):\n",
    "    \"\"\"Randomly sample points from a two-dimensional Gaussian distribution\"\"\"\n",
    "    np.random.seed(142)\n",
    "    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[variance, cov], [cov, variance]]), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random = create_2D_gaussian(mn=50, variance=1, cov=0, n=100)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\n",
    "plt.scatter(data_random[:,0], data_random[:,1], s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_correlated = create_2D_gaussian(mn=50, variance=1, cov=.9, n=100)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.scatter(data_correlated[:,0], data_correlated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Interpreting PCA\n",
    "\n",
    "PCA can be interpreted as identifying the \"directions\" along which the data vary the most. In the first step of PCA, we must first center our data.  Working with our correlated dataset, first compute the mean of each feature (column) in the dataset.  Then for each observation, modify the features by subtracting their corresponding mean, to create a zero mean dataset.\n",
    "\n",
    "> Note:\n",
    "> * `correlated_data` is an RDD of NumPy arrays.\n",
    "> * This allows us to perform certain operations more succinctly.\n",
    "> * For example, we can sum the columns of our dataset using `correlated_data.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a544de710c823695a976f7bb1cda810d",
     "grade": false,
     "grade_id": "answer_interpretePCA1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to interprete PCA\n",
    "correlated_data = sc.parallelize(data_correlated)\n",
    "\n",
    "# # Interpreting PCA, first compute the mean and subtract it from the original feature\n",
    "# mean_correlated = <FILL IN>\n",
    "# correlated_data_zero_mean = correlated_data.<FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(mean_correlated)\n",
    "print(correlated_data.take(1))\n",
    "print(correlated_data_zero_mean.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57f0af873c1b603614e100e2083e45c9",
     "grade": true,
     "grade_id": "test_interpretePCA1a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Interpreting PCA (1a)\n",
    "from nose.tools import assert_true, assert_equal\n",
    "assert_true(np.allclose(mean_correlated, [49.95739037, 49.97180477], atol=1e-2),\n",
    "                'incorrect value for mean_correlated')\n",
    "assert_true(np.allclose(correlated_data_zero_mean.take(1)[0], [-0.28561917, 0.10351492], atol=1e-2),\n",
    "                'incorrect value for correlated_data_zero_mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Sample covariance matrix\n",
    "\n",
    "We are now ready to compute the sample covariance matrix. If we define \\\\(\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\\\) as the zero mean data matrix, then the sample covariance matrix is defined as: \\\\[ \\mathbf{C}_{\\mathbf X} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.\\\\]\n",
    "\n",
    "To compute this matrix, compute the outer product of each data point, add together these outer products, and divide by the number of data points. The data are two dimensional, so the resulting covariance matrix should be a 2x2 matrix.\n",
    "\n",
    "> Note:\n",
    "> * [np.outer()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) can be used to calculate the outer product of two NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e7557dc2eeb8208f4429a57bf3c3d64",
     "grade": false,
     "grade_id": "answer_covMatrix1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to compute cov matrix\n",
    "# Compute the covariance matrix using outer products\n",
    "# # 1. Compute the outer product of each data point\n",
    "# # 2. Add together these outer products\n",
    "# # 3. Divide by the number of data points\n",
    "\n",
    "# correlated_cov = (correlated_data_zero_mean\n",
    "#                  .map(<FILL_IN>)\n",
    "#                  .<FILL_IN> ) / correlated_data_zero_mean.<FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(correlated_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18da622a665bec45ab2746377a7da8af",
     "grade": true,
     "grade_id": "test_covMatrix1b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sample covariance matrix (1b)\n",
    "cov_result = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "assert_true(np.allclose(cov_result, correlated_cov, atol=1e-2), 'incorrect value for correlated_cov')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Covariance Function\n",
    "\n",
    "Next, use the expressions above to write a function to compute the sample covariance matrix for an arbitrary `data` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1c61707734c2e0e01eaa56a3ff89f2b",
     "grade": false,
     "grade_id": "answer_covFunction1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to compute the cov matrix\n",
    "# # Write a function to compute the sample covariance matrix for an arbitrary RDD\n",
    "def estimate_covariance(data):\n",
    "    \"\"\"Compute the covariance matrix for a given rdd.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input `RDD`.\n",
    "    \"\"\"\n",
    "#     n = <FILL_IN>\n",
    "#     mean = <FILL_IN>\n",
    "#     data_zero_mean = <FILL_IN>\n",
    "#     sample_cov = <FILL_IN>\n",
    "#     return <FILL_IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "\n",
    "\n",
    "correlated_cov_auto= estimate_covariance(correlated_data)\n",
    "print(correlated_cov_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fe508a64b160956ef09a13a8acd529f",
     "grade": true,
     "grade_id": "test_covFunction1c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Covariance function (1c)\n",
    "correct_cov = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "assert_true(np.allclose(correct_cov, correlated_cov_auto, atol=1e-2),\n",
    "                'incorrect value for correlated_cov_auto')\n",
    "\n",
    "test_data = np.array([[0,1,2,3], [4,5,6,7], [8,9,10,11], [12,13,14,15]])\n",
    "cov_test_data = sc.parallelize(test_data)\n",
    "correct_test_cov = [[20., 20., 20., 20.],\n",
    "                    [ 20.,  20.,  20.,  20.],\n",
    "                    [ 20.,  20.,  20.,  20.],\n",
    "                    [ 20.,  20.,  20.,  20.]]\n",
    "assert_true(np.allclose(correct_test_cov, estimate_covariance(cov_test_data), atol=1e-2), 'incorrect value returned by estimate_covariance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Eigendecomposition\n",
    "\n",
    "Now that we've computed the sample covariance matrix, we can use it to find directions of maximal variance in the data.  Specifically, we can perform an eigendecomposition of this matrix to find its eigenvalues and eigenvectors.  The \\\\(\\scriptsize d \\\\) eigenvectors of the covariance matrix give us the directions of maximal variance, and are often called the \"principal components.\"  The associated eigenvalues are the variances in these directions.  In particular, the eigenvector corresponding to the largest eigenvalue is the direction of maximal variance (this is sometimes called the \"top\" eigenvector). Eigendecomposition of a \\\\(\\scriptsize d \\times d \\\\) covariance matrix has a (roughly) cubic runtime complexity with respect to \\\\(\\scriptsize d \\\\).  Whenever \\\\(\\scriptsize d \\\\) is relatively small (e.g., less than a few thousand) we can quickly perform this eigendecomposition locally.\n",
    "\n",
    "Use a function from `numpy.linalg` called [eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) to perform the eigendecomposition.  Next, sort the eigenvectors based on their corresponding eigenvalues (from high to low), yielding a matrix where the columns are the eigenvectors (and the first column is the top eigenvector).  Note that [np.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy-argsort) can be used to obtain the indices of the eigenvalues that correspond to the ascending order of eigenvalues.  Finally, set the `top_component` variable equal to the top eigenvector or prinicipal component, which is a \\\\(\\scriptsize 2 \\\\)-dimensional vector (array with two values).\n",
    "\n",
    "> Note:\n",
    "> * The eigenvectors returned by `eigh` appear in the columns and not the rows.\n",
    "> * For example, the first eigenvector of `eig_vecs` would be found in the first column and could be accessed using `eig_vecs[:,0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca78ca4ffc112737b66c94094c465b49",
     "grade": false,
     "grade_id": "answer_eigenDecomposition1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to calculate eigenvalues and eigenvectors\n",
    "from numpy.linalg import eigh\n",
    "\n",
    "# # Calculate the eigenvalues and eigenvectors from correlated_cov_auto\n",
    "\n",
    "# eig_vals, eig_vecs = <FILL_IN>\n",
    "# # Use np.argsort to find the top eigenvector based on the largest eigenvalue\n",
    "\n",
    "# inds = np.argsort(<FILL_IN>)\n",
    "# top_component = <FILL_IN>\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('eigenvalues: {0}'.format(eig_vals))\n",
    "print('\\neigenvectors: \\n{0}'.format(eig_vecs))\n",
    "print('\\ntop principal component: {0}'.format(top_component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07ea4e8493e89df3156a2768697f9d2d",
     "grade": true,
     "grade_id": "test_eigenDecomposition1d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Eigendecomposition (1d)\n",
    "def check_basis(vectors, correct):\n",
    "    return np.allclose(vectors, correct) or np.allclose(np.negative(vectors), correct)\n",
    "\n",
    "assert_true(check_basis(top_component, [0.68915649, 0.72461254]),\n",
    "                'incorrect value for top_component')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) PCA scores\n",
    "\n",
    "We just computed the top principal component for a 2-dimensional non-spherical dataset.  Now let's use this principal component to derive a one-dimensional representation for the original data. To compute these compact representations, which are sometimes called PCA \"scores\", calculate the dot product between each data point in the raw data and the top principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4db15d69d25de2379f64ee791f594b80",
     "grade": false,
     "grade_id": "answer_PCAscores1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to generate PCA scores using the top_component\n",
    "# Use the top_component and the data from correlated_data to generate PCA scores\n",
    "\n",
    "# correlated_data_scores = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('one-dimensional data (first three):\\n{0}'.format(np.asarray(correlated_data_scores.take(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b848a18f05187c1c6b296e5dd07b097a",
     "grade": true,
     "grade_id": "test_PCAscores1e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA Scores (1e)\n",
    "first_three = [70.51682806, 69.30622356, 71.13588168]\n",
    "assert_true(check_basis(correlated_data_scores.take(3), first_three),\n",
    "                'incorrect value for correlated_data_scores')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Write a PCA function and evaluate PCA on sample datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) PCA function\n",
    "\n",
    "We now have all the ingredients to write a general PCA function.  Instead of working with just the top principal component, our function will compute the top \\\\(\\scriptsize k\\\\) principal components and principal scores for a given dataset. The top \\\\(\\scriptsize k\\\\) principal components should be returned in descending order when ranked by their corresponding principal scores. Write this general function `pca`, and run it with `correlated_data` and \\\\(\\scriptsize k = 2\\\\). Hint: Use results from Part (1c), Part (1d), and Part (1e).\n",
    "\n",
    "Note: As discussed in lecture, our implementation is a reasonable strategy when \\\\(\\scriptsize d \\\\) is small, though more efficient distributed algorithms exist when \\\\(\\scriptsize d \\\\) is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31270afee5f7352edb8a158dee85f31d",
     "grade": false,
     "grade_id": "answer_PCAfunction2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to compute top-k principle components, scores and all eigenvalues\n",
    "\n",
    "def pca(data, k=2):\n",
    "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
    " \n",
    "    Args:\n",
    "        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n",
    "        k (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
    "            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
    "            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
    "            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
    "            of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
    "    \"\"\"\n",
    "    # sample_cov = <FILL_IN>\n",
    "    # eig_vals, eig_vecs = <FILL_IN>\n",
    "    # inds = <FILL_IN>\n",
    "    # # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "    # components = <FILL_IN>\n",
    "    # eigenvalues = <FILL_IN\n",
    "    # scores = data.map(<FILL_IN>)\n",
    "    # return <FILL_IN>\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Run pca on correlated_data with k = 2\n",
    "top_components_correlated, correlated_data_scores_auto, eigenvalues_correlated  = pca(correlated_data, 2)\n",
    "\n",
    "# Note that the 1st principal component is in the first column\n",
    "print('top_components_correlated: \\n{0}'.format(top_components_correlated))\n",
    "print('\\ncorrelated_data_scores_auto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, correlated_data_scores_auto.take(3)))))\n",
    "print('\\neigenvalues_correlated: \\n{0}'.format(eigenvalues_correlated))\n",
    "\n",
    "# Create a higher dimensional test set\n",
    "pca_test_data = sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\n",
    "components_test, test_scores, eigenvalues_test = pca(pca_test_data, 3)\n",
    "\n",
    "print('\\npca_test_data: \\n{0}'.format(np.array(pca_test_data.collect())))\n",
    "print('\\ncomponents_test: \\n{0}'.format(components_test))\n",
    "print('\\ntest_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, test_scores.take(3)))))\n",
    "print('\\neigenvalues_test: \\n{0}'.format(eigenvalues_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a791a224bc82d797e63ae091e28b7095",
     "grade": true,
     "grade_id": "test_PCAfunction2a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA Function (2a)\n",
    "assert_true(check_basis(top_components_correlated.T,\n",
    "                            [[0.68915649,  0.72461254], [-0.72461254, 0.68915649]]),\n",
    "                'incorrect value for top_components_correlated')\n",
    "first_three_correlated = [[70.51682806, 69.30622356, 71.13588168], [1.48305648, 1.5888655, 1.86710679]]\n",
    "assert_true(np.allclose(first_three_correlated,\n",
    "                            np.vstack(np.abs(correlated_data_scores_auto.take(3))).T, atol=1e-2),\n",
    "                'incorrect value for first three correlated values')\n",
    "assert_true(np.allclose(eigenvalues_correlated, [1.94345403, 0.13820481], atol=1e-2),\n",
    "                           'incorrect values for eigenvalues_correlated')\n",
    "top_components_correlated_k1, correlated_data_scores_k1, eigenvalues_correlated_k1 = pca(correlated_data, 1)\n",
    "assert_true(check_basis(top_components_correlated_k1.T, [0.68915649, 0.72461254]),\n",
    "               'incorrect value for components when k=1')\n",
    "assert_true(np.allclose([70.51682806, 69.30622356, 71.13588168],\n",
    "                            np.vstack(np.abs(correlated_data_scores_k1.take(3))).T, atol=1e-2),\n",
    "                'incorrect value for scores when k=1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) PCA on `data_random`\n",
    "\n",
    "Next, use the PCA function we just developed to find the top two principal components of the spherical `data_random` we created in Visualization 1.\n",
    "\n",
    "First, we need to convert `data_random` to the RDD `random_data_rdd`, and do all subsequent operations on `random_data_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "952d9cfd379c0c250f2dc9d6b4de47d3",
     "grade": false,
     "grade_id": "answer_PCADataRandom2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to convert data_random to RDD random_data_rdd\n",
    "random_data_rdd = sc.parallelize(data_random)\n",
    "\n",
    "# # Use pca on data_random\n",
    "# top_components_random, random_data_scores_auto, eigenvalues_random = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('top_components_random: \\n{0}'.format(top_components_random))\n",
    "print('\\nrandom_data_scores_auto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, random_data_scores_auto.take(3)))))\n",
    "print('\\neigenvalues_random: \\n{0}'.format(eigenvalues_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55eb47b35fc9b87d364220e3fa58d3d7",
     "grade": true,
     "grade_id": "test_PCADataRandom2b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA on `data_random` (2b)\n",
    "assert_true(check_basis(top_components_random.T,\n",
    "                            [[-0.2522559 ,  0.96766056], [-0.96766056,  -0.2522559]]),\n",
    "                'incorrect value for top_components_random')\n",
    "first_three_random = [[36.61068572, 35.97314295, 35.59836628],\n",
    "                      [61.3489929 ,  62.08813671,  60.61390415]]\n",
    "assert_true(np.allclose(first_three_random, np.vstack(np.abs(random_data_scores_auto.take(3))).T, atol=1e-2),\n",
    "                'incorrect value for random_data_scores_auto')\n",
    "assert_true(np.allclose(eigenvalues_random, [1.4204546, 0.99521397], atol=1e-2),\n",
    "                            'incorrect value for eigenvalues_random')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: PCA projection\n",
    "\n",
    "Plot the original data and the 1-dimensional reconstruction using the top principal component to see how the PCA solution looks.  The original data is plotted as before; however, the 1-dimensional reconstruction (projection) is plotted in green on top of the original data and the vectors (lines) representing the two principal components are shown as dotted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points_and_get_lines(data, components, x_range):\n",
    "    \"\"\"Project original data onto first component and get line details for top two components.\"\"\"\n",
    "    top_component = components[:, 0]\n",
    "    slope1, slope2 = components[1, :2] / components[0, :2]\n",
    "\n",
    "    means = data.mean()[:2]\n",
    "    demeaned = data.map(lambda v: v - means)\n",
    "    projected = demeaned.map(lambda v: (v.dot(top_component) /\n",
    "                                        top_component.dot(top_component)) * top_component)\n",
    "    remeaned = projected.map(lambda v: v + means)\n",
    "    x1,x2 = zip(*remeaned.collect())\n",
    "\n",
    "    line_start_P1_X1, line_start_P1_X2 = means - np.asarray([x_range, x_range * slope1])\n",
    "    line_end_P1_X1, line_end_P1_X2 = means + np.asarray([x_range, x_range * slope1])\n",
    "    line_start_P2_X1, line_start_P2_X2 = means - np.asarray([x_range, x_range * slope2])\n",
    "    line_end_P2_X1, line_end_P2_X2 = means + np.asarray([x_range, x_range * slope2])\n",
    "\n",
    "    return ((x1, x2), ([line_start_P1_X1, line_end_P1_X1], [line_start_P1_X2, line_end_P1_X2]),\n",
    "            ([line_start_P2_X1, line_end_P2_X1], [line_start_P2_X2, line_end_P2_X2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    project_points_and_get_lines(correlated_data, top_components_correlated, 5)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(data_correlated[:,0], data_correlated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    project_points_and_get_lines(random_data_rdd, top_components_random, 5)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(data_random[:,0], data_random[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Three-dimensional data\n",
    "\n",
    "So far we have worked with two-dimensional data. Now let's generate three-dimensional data with highly correlated features. As in Visualization 1, we'll create samples from a multivariate Gaussian distribution, which in three dimensions requires us to specify three means, three variances, and three covariances.\n",
    "\n",
    "In the 3D graphs below, we have included the 2D plane that corresponds to the top two principal components, i.e. the plane with the smallest euclidean distance between the points and itself. Notice that the data points, despite living in three-dimensions, are found near a two-dimensional plane: the left graph shows how most points are close to the plane when it is viewed from its side, while the right graph shows that the plane covers most of the variance in the data.  Note that darker blues correspond to points with higher values for the third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "m = 100\n",
    "mu = np.array([50, 50, 50])\n",
    "r1_2 = 0.9\n",
    "r1_3 = 0.7\n",
    "r2_3 = 0.1\n",
    "sigma1 = 5\n",
    "sigma2 = 20\n",
    "sigma3 = 20\n",
    "c = np.array([[sigma1 ** 2, r1_2 * sigma1 * sigma2, r1_3 * sigma1 * sigma3],\n",
    "             [r1_2 * sigma1 * sigma2, sigma2 ** 2, r2_3 * sigma2 * sigma3],\n",
    "             [r1_3 * sigma1 * sigma3, r2_3 * sigma2 * sigma3, sigma3 ** 2]])\n",
    "np.random.seed(142)\n",
    "data_threeD = np.random.multivariate_normal(mu, c, m)\n",
    "\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "norm = Normalize()\n",
    "cmap = get_cmap(\"Blues\")\n",
    "clrs = cmap(np.array(norm(data_threeD[:,2])))[:,0:3]\n",
    "\n",
    "fig = plt.figure(figsize=(11, 6))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.azim=-100\n",
    "ax.scatter(data_threeD[:,0], data_threeD[:,1], data_threeD[:,2], c=clrs, s=14**2)\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(-15, 10, 1), np.arange(-50, 30, 1))\n",
    "normal = np.array([0.96981815, -0.188338, -0.15485978])\n",
    "z = (-normal[0] * xx - normal[1] * yy) * 1. / normal[2]\n",
    "xx = xx + 50\n",
    "yy = yy + 50\n",
    "z = z + 50\n",
    "\n",
    "ax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\n",
    "ax.plot_surface(xx, yy, z, alpha=.10)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.azim=10\n",
    "ax.elev=20\n",
    "#ax.dist=8\n",
    "ax.scatter(data_threeD[:,0], data_threeD[:,1], data_threeD[:,2], c=clrs, s=14**2)\n",
    "\n",
    "ax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\n",
    "ax.plot_surface(xx, yy, z, alpha=.1)\n",
    "plt.tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) 3D to 2D\n",
    "\n",
    "We will now use PCA to see if we can recover the 2-dimensional plane on which the data live. Parallelize the data, and use our PCA function from above, with \\\\( \\scriptsize k=2 \\\\) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d819c4a9de906d450ba85a09d1b6f06",
     "grade": false,
     "grade_id": "answer_PCAThreeDimension2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to apply PCA function to 3D data. You need to first parallelize the data and use the PCA function defined above with k=2 components\n",
    "threeD_data = sc.parallelize(data_threeD)\n",
    "\n",
    "# components_threeD, threeD_scores, eigenvalues_threeD = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('components_threeD: \\n{0}'.format(components_threeD))\n",
    "print('\\nthreeD_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, threeD_scores.take(3)))))\n",
    "print('\\neigenvalues_threeD: \\n{0}'.format(eigenvalues_threeD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b500f897fcd2aff8298e7ec85e1137f7",
     "grade": true,
     "grade_id": "test_PCAThreeDimension2c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST 3D to 2D (2c)\n",
    "assert_equal(components_threeD.shape, (3, 2), 'incorrect shape for components_threeD')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_threeD), 969.796443367, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_threeD')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_threeD)), 1.77238943258, atol=1e-2),\n",
    "                'incorrect value for components_threeD')\n",
    "assert_true(np.allclose(np.abs(np.sum(threeD_scores.take(3))), 237.782834092, atol=1e-2),\n",
    "                'incorrect value for threeD_scores')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: 2D representation of 3D data\n",
    "\n",
    "See the 2D version of the data that captures most of its original structure.  Note that darker blues correspond to points with higher values for the original data's third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_threeD = np.asarray(threeD_scores.collect())\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(20, 150, 20), np.arange(-40, 110, 20))\n",
    "ax.set_xlabel(r'New $x_1$ values'), ax.set_ylabel(r'New $x_2$ values')\n",
    "ax.set_xlim(5, 150), ax.set_ylim(-45, 50)\n",
    "plt.scatter(scores_threeD[:, 0], scores_threeD[:, 1], s=14 ** 2, c=clrs, edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Variance explained\n",
    "\n",
    "Finally, let's quantify how much of the variance is being captured by PCA in each of the three synthetic datasets we've analyzed.  To do this, we'll compute the fraction of retained variance by the top principal components.  Recall that the eigenvalue corresponding to each principal component captures the variance along this direction.  If our initial data is \\\\(\\scriptsize d\\\\)-dimensional, then the total variance in our data equals: \\\\( \\scriptsize \\sum_{i=1}^d \\lambda_i \\\\), where \\\\(\\scriptsize \\lambda_i\\\\) is the eigenvalue corresponding to the \\\\(\\scriptsize i\\\\)th principal component. Moreover, if we use PCA with some \\\\(\\scriptsize k < d\\\\), then we can compute the variance retained by these principal components by adding the top \\\\(\\scriptsize k\\\\) eigenvalues.  The fraction of retained variance equals the sum of the top \\\\(\\scriptsize k\\\\) eigenvalues divided by the sum of all of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5013f363cdd75d07bc5cc6838b61da94",
     "grade": false,
     "grade_id": "answer_varianceExplained2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to compute the fraction of retained variance by the top principle components\n",
    "\n",
    "def variance_explained(data, k=1):\n",
    "    \"\"\"Calculate the fraction of variance explained by the top `k` eigenvectors.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray): An RDD that contains NumPy arrays which store the\n",
    "            features for an observation.\n",
    "        k: The number of principal components to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: A number between 0 and 1 representing the percentage of variance explained\n",
    "            by the top `k` eigenvectors.\n",
    "    \"\"\"\n",
    "    # components, scores, eigenvalues = <FILL IN>\n",
    "    # variances = <FILL IN>\n",
    "    # return <FILL IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "variance_random_1 = variance_explained(random_data_rdd, 1)\n",
    "variance_correlated_1 = variance_explained(correlated_data, 1)\n",
    "variance_random_2 = variance_explained(random_data_rdd, 2)\n",
    "variance_correlated_2 = variance_explained(correlated_data, 2)\n",
    "variance_threeD_2 = variance_explained(threeD_data, 2)\n",
    "print ('Percentage of variance explained by the first component of random_data_rdd: {0:.1f}%'\n",
    "       .format(variance_random_1 * 100))\n",
    "print ('Percentage of variance explained by both components of random_data_rdd: {0:.1f}%'\n",
    "       .format(variance_random_2 * 100))\n",
    "print ('\\nPercentage of variance explained by the first component of correlated_data: {0:.1f}%'.\n",
    "       format(variance_correlated_1 * 100))\n",
    "print ('Percentage of variance explained by both components of correlated_data: {0:.1f}%'\n",
    "       .format(variance_correlated_2 * 100))\n",
    "print ('\\nPercentage of variance explained by the first two components of threeD_data: {0:.1f}%'\n",
    "       .format(variance_threeD_2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f392e5c9ae814514b4d338c5a914e773",
     "grade": true,
     "grade_id": "test_varianceExplained2d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Variance explained (2d)\n",
    "assert_true(np.allclose(variance_random_1, 0.588017172066, atol=1e-2), 'incorrect value for variance_random_1')\n",
    "assert_true(np.allclose(variance_correlated_1, 0.933608329586, atol=1e-2),\n",
    "                'incorrect value for varianceCorrelated1')\n",
    "assert_true(np.allclose(variance_random_2, 1.0, atol=1e-2), 'incorrect value for variance_random_2')\n",
    "assert_true(np.allclose(variance_correlated_2, 1.0, atol=1e-2), 'incorrect value for variance_correlated_2')\n",
    "assert_true(np.allclose(variance_threeD_2, 0.993967356912, atol=1e-2), 'incorrect value for variance_threeD_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Parse, inspect, and preprocess neuroscience data then perform PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data introduction\n",
    "\n",
    "A central challenge in neuroscience is understanding the organization and function of neurons, the cells responsible for processing and representing information in the brain. New technologies make it possible to monitor the responses of large populations of neurons in awake animals. In general, neurons communicate through electrical impulses that must be recorded with electrodes, which is a challenging process. As an alternative, we can genetically engineer animals so that their neurons express special proteins that fluoresce or light up when active, and then use microscopy to record neural activity as images.\n",
    "\n",
    "Light-sheet microscopy lets us do this in a special, transparent animal, the larval zebrafish, over nearly its entire brain. The resulting data are time-varying images containing the activity of hundreds of thousands of neurons. Given the raw data, which is enormous, we want to find compact spatial and temporal patterns: Which groups of neurons are active together? What is the time course of their activity? Are those patterns specific to particular events happening during the experiment (e.g. a stimulus that we might present). PCA is a powerful technique for finding spatial and temporal patterns in these kinds of data, and that's what we'll explore here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Load neuroscience data\n",
    "\n",
    "In the next sections we will use PCA to capture structure in neural datasets. Before doing the analysis, we will load and do some basic inspection of the data. The raw data are currently stored as a text file. Every line in the file contains the time series of image intensity for a single pixel in a time-varying image (i.e. a movie). The first two numbers in each line are the spatial coordinates of the pixel, and the remaining numbers are the time series. We'll use `first()` to inspect a single row, and print just the first 100 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/10605/data/master/hw3/neuro.txt'\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "sc.addFile(url)\n",
    "\n",
    "lines = sc.textFile(\"file://\" + SparkFiles.get(\"neuro.txt\"))\n",
    "print(lines.first()[0:100])\n",
    "\n",
    "# Check that everything loaded properly\n",
    "assert len(lines.first()) == 1397\n",
    "assert lines.count() == 46460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Parse the data\n",
    "\n",
    "Parse the data into a key-value representation. We want each key to be a tuple of two-dimensional spatial coordinates and each value to be a NumPy array storing the associated time series. Write a function that converts a line of text into a (`tuple`, `np.ndarray`) pair. Then apply this function to each record in the RDD, and inspect the first entry of the new parsed data set. Now would be a good time to cache the data, and force a computation by calling count, to ensure the data are cached.\n",
    "\n",
    "Note: `tuple` contains 2 integer values and the numpy array consists of float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b57aaba046fedccacd6bc19cd514bfc6",
     "grade": false,
     "grade_id": "answer_parseData3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to parse the data into (`tuple`, `np.ndarray`) format \n",
    "\n",
    "def parse(line):\n",
    "    \"\"\"Parse the raw data into a (`tuple`, `np.ndarray`) pair.\n",
    "\n",
    "    Note:\n",
    "        You should store the pixel coordinates as a tuple of two ints and the elements of the pixel intensity\n",
    "        time series as an np.ndarray of floats.\n",
    "\n",
    "    Args:\n",
    "        line (str): A string representing an observation.  Elements are separated by spaces.  The\n",
    "            first two elements represent the coordinates of the pixel, and the rest of the elements\n",
    "            represent the pixel intensity over time.\n",
    "\n",
    "    Returns:\n",
    "        tuple of tuple, np.ndarray: A (coordinate, pixel intensity array) `tuple` where coordinate is\n",
    "            a `tuple` containing two values and the pixel intensity is stored in an NumPy array\n",
    "            which contains 240 values.\n",
    "    \"\"\"\n",
    "    # vec = <FILL_IN>\n",
    "    # key = <FILL_IN>\n",
    "    # ts = <FILL_IN>\n",
    "    # return <FILL_IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "raw_data = lines.map(parse)\n",
    "raw_data.cache()\n",
    "entry = raw_data.first()\n",
    "print ('Length of movie is {0} seconds'.format(len(entry[1])))\n",
    "print ('Number of pixels in movie is {0:,}'.format(raw_data.count()))\n",
    "print ('\\nFirst entry of raw_data (with only the first five values of the NumPy array):\\n({0}, {1})'\n",
    "       .format(entry[0], entry[1][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf4ed8142f312707a664f657fcc1e0d5",
     "grade": true,
     "grade_id": "test_parseData3b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Parse the data (3b)\n",
    "assert_true(isinstance(entry[0], tuple), \"entry's key should be a tuple\")\n",
    "assert_equal(len(entry), 2, 'entry should have a key and a value')\n",
    "assert_true(isinstance(entry[0][1], int), 'coordinate tuple should contain ints')\n",
    "assert_equal(len(entry[0]), 2, \"entry's key should have two values\")\n",
    "assert_true(isinstance(entry[1], np.ndarray), \"entry's value should be an np.ndarray\")\n",
    "assert_true(isinstance(entry[1][0], np.float), 'the np.ndarray should consist of np.float values')\n",
    "assert_equal(entry[0], (0, 0), 'incorrect key for entry')\n",
    "assert_equal(entry[1].size, 240, 'incorrect length of entry array')\n",
    "assert_true(np.allclose(np.sum(entry[1]), 24683.5), 'incorrect values in entry array')\n",
    "assert_true(raw_data.is_cached, 'raw_data is not cached')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Min and max fluorescence\n",
    "\n",
    "Next we'll do some basic preprocessing on the data. The raw time-series data are in units of image fluorescence, and baseline fluorescence varies somewhat arbitrarily from pixel to pixel. First, compute the minimum and maximum values across all pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e8846b322ee6fe551dbe9671bb0a27f",
     "grade": false,
     "grade_id": "answer_MinMax3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to calculate min and max\n",
    "# mn = raw_data.<FILL_IN>\n",
    "# mx = raw_data.<FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print (mn, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6c5b3f8049ece3a99c3c763810f1533",
     "grade": true,
     "grade_id": "test_MinMax3c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Min and max fluorescence (3c)\n",
    "assert_true(np.allclose(mn, 100.6, atol=1e-2), 'incorrect value for mn')\n",
    "assert_true(np.allclose(mx, 940.8, atol=1e-2), 'incorrect value for mx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 5: Pixel intensity\n",
    "\n",
    "Let's now see how a random pixel varies in value over the course of the time series.  We'll visualize a pixel that exhibits a standard deviation of over 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = raw_data.filter(lambda x: np.std(x[1]) > 100).values().first()\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 300, 50), np.arange(300, 800, 100))\n",
    "ax.set_xlabel(r'time'), ax.set_ylabel(r'fluorescence')\n",
    "ax.set_xlim(-20, 270), ax.set_ylim(270, 730)\n",
    "plt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Fractional signal change\n",
    "\n",
    "To convert from these raw fluorescence units to more intuitive units of fractional signal change, write a function that takes a time series for a particular pixel and subtracts and divides by the mean.  Then apply this function to all the pixels. Confirm that this changes the maximum and minimum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63f5e654f2205a63e56dac9699cd6579",
     "grade": false,
     "grade_id": "answer_rescale3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to standardize the array\n",
    "\n",
    "def rescale(ts):\n",
    "    \"\"\"Take a np.ndarray and return the standardized array by subtracting and dividing by the mean.\n",
    "\n",
    "    Note:\n",
    "        You should first subtract the mean and then divide by the mean.\n",
    "\n",
    "    Args:\n",
    "        ts (np.ndarray): Time series data (`np.float`) representing pixel intensity.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The times series adjusted by subtracting the mean and dividing by the mean.\n",
    "    \"\"\"\n",
    "    # return <FILL_IN>\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "scaled_data = raw_data.mapValues(lambda v: rescale(v))\n",
    "mn_scaled = scaled_data.map(lambda x: x[1]).map(lambda v: min(v)).min()\n",
    "mx_scaled = scaled_data.map(lambda x: x[1]).map(lambda v: max(v)).max()\n",
    "print(mn_scaled, mx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf9d7628dba52dfe607d0eff250b2e2b",
     "grade": true,
     "grade_id": "test_rescale3d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Fractional signal change (3d)\n",
    "assert_true(isinstance(scaled_data.first()[1], np.ndarray), 'incorrect type returned by rescale')\n",
    "assert_true(np.allclose(mn_scaled, -0.27151288, atol=1e-2), 'incorrect value for mn_scaled')\n",
    "assert_true(np.allclose(mx_scaled, 0.90544876, atol=1e-2), 'incorrect value for mx_scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 6: Normalized data\n",
    "\n",
    "Now that we've normalized our data, let's once again see how a random pixel varies in value over the course of the time series.  We'll visualize a pixel that exhibits a standard deviation of over 0.1.  Note the change in scale on the y-axis compared to the previous visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = scaled_data.filter(lambda x: np.std(x[1]) > 0.1).values().first()\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 300, 50), np.arange(-.1, .6, .1))\n",
    "ax.set_xlabel(r'time'), ax.set_ylabel(r'fluorescence')\n",
    "ax.set_xlim(-20, 260), ax.set_ylim(-.12, .52)\n",
    "plt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) PCA on the scaled data\n",
    "\n",
    "We now have a preprocessed dataset with \\\\(\\scriptsize n = 46460\\\\) pixels and \\\\(\\scriptsize d = 240\\\\) seconds of time series data for each pixel.  We can interpret the pixels as our observations and each pixel value in the time series as a feature.  We would like to find patterns in brain activity during this time series, and we expect to find correlations over time.  We can thus use PCA to find a more compact representation of our data and allow us to visualize it.\n",
    "\n",
    "Use the `pca` function from Part (2a) to perform PCA on the preprocessed neuroscience data with \\\\(\\scriptsize k = 3\\\\), resulting in a new low-dimensional 46460 by 3 dataset.  The `pca` function takes an RDD of arrays, but `scaled_data` is an RDD of key-value pairs, so you'll need to extract the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9723942b0e2337bd8d5062cde1664af8",
     "grade": false,
     "grade_id": "answer_PCAscaled3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to run pca using scaled_data\n",
    "\n",
    "# components_scaled, scaled_scores, eigenvalues_scaled = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('components_scaled: (first five) \\n{0}'.format(components_scaled[:5, :]))\n",
    "print ('\\nscaled_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, scaled_scores.take(3)))))\n",
    "print ('\\neigenvalues_scaled: (first five) \\n{0}'.format(eigenvalues_scaled[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f69a6f3312dc2c4faff95974ca3545ad",
     "grade": true,
     "grade_id": "test_PCAscaled3e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA on the scaled data (3e)\n",
    "assert_equal(components_scaled.shape, (240, 3), 'incorrect shape for components_scaled')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_scaled[:5, :])), 0.283150995232, atol=1e-2),\n",
    "                'incorrect value for components_scaled')\n",
    "assert_true(np.allclose(np.abs(np.sum(scaled_scores.take(3))), 0.0285507449251, atol=1e-2),\n",
    "                'incorrect value for scaled_scores')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_scaled[:5]), 0.206987501564, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 7: Top two components as images\n",
    "\n",
    "Now, we'll view the scores for the top two components as images.  Note that we reshape the vectors by the dimensions of the original image, 230 x 202.\n",
    "These graphs map the values for the single component to a grayscale image.  This provides us with a visual representation which we can use to see the overall structure of the zebrafish brain and to identify where high and low values occur.  However, using this representation, there is a substantial amount of useful information that is difficult to interpret.  In the next visualization, we'll see how we can improve interpretability by combining the two principal components into a single image using a color mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "scores_scaled = np.vstack(scaled_scores.collect())\n",
    "image_one_scaled = scores_scaled[:, 0].reshape(230, 202).T\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "ax.set_title('Top Principal Component', color='#888888')\n",
    "image = plt.imshow(image_one_scaled, interpolation='nearest', aspect='auto', cmap=cm.gray)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_two_scaled = scores_scaled[:, 1].reshape(230, 202).T\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "ax.set_title('Second Principal Component', color='#888888')\n",
    "image = plt.imshow(image_two_scaled, interpolation='nearest', aspect='auto', cmap=cm.gray)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 8: Top two components as one image\n",
    "\n",
    "When we perform PCA and color neurons based on their location in the low-dimensional space, we can interpret areas with similar colors as exhibiting similar responses (at least in terms of the simple representation we recover with PCA). Below, the first graph shows how low-dimensional representations, which correspond to the first two principal components, are mapped to colors. The second graph shows the result of this color mapping using the zebrafish neural data.\n",
    "\n",
    "The second graph clearly exhibits patterns of neural similarity throughout different regions of the brain.  However, when performing PCA on the full dataset, there are multiple reasons why neurons might have similar responses. The neurons might respond similarly to different stimulus directions, their responses might have  similar temporal dynamics, or their response similarity could be influenced by both temporal and stimulus-specific factors. However, with our initial PCA analysis, we cannot pin down the underlying factors, and hence it is hard to interpret what \"similarity\" really means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Details: Note that we use [polar coordinates](https://en.wikipedia.org/wiki/Polar_coordinate_system) to map our low-dimensional points to colors.  Using polar coordinates provides us with an angle \\\\( (\\phi) \\\\) and magnitude \\\\( (\\rho) \\\\).  We then use the well-known polar color space, [hue-saturation-value](https://en.wikipedia.org/wiki/HSL_and_HSV) (HSV), and map the angle to hue and the magnitude to value (brightness).  This maps low magnitude points to black while allowing larger magnitude points to be differentiated by their angle. Additionally, the function `polarTransform` that maps low-dimensional representations to colors has an input parameter called `scale`, which we set  to 2.0, and you can try lower values for the two graphs to see more nuanced mappings -- values near 1.0 are particularly interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from python-thunder's Colorize.transform where cmap='polar'.\n",
    "# Checkout the library at: https://github.com/thunder-project/thunder and\n",
    "# http://thunder-project.org/\n",
    "\n",
    "def polar_transform(scale, img):\n",
    "    \"\"\"Convert points from cartesian to polar coordinates and map to colors.\"\"\"\n",
    "    from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "    img = np.asarray(img)\n",
    "    dims = img.shape\n",
    "\n",
    "    phi = ((np.arctan2(-img[0], -img[1]) + np.pi/2) % (np.pi*2)) / (2 * np.pi)\n",
    "    rho = np.sqrt(img[0]**2 + img[1]**2)\n",
    "    saturation = np.ones((dims[1], dims[2]))\n",
    "\n",
    "    out = hsv_to_rgb(np.dstack((phi, saturation, scale * rho)))\n",
    "\n",
    "    return np.clip(out * scale, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the polar mapping from principal component coordinates to colors.\n",
    "x1_abs_max = np.max(np.abs(image_one_scaled))\n",
    "x2_abs_max = np.max(np.abs(image_two_scaled))\n",
    "\n",
    "num_of_pixels = 300\n",
    "x1_vals = np.arange(-x1_abs_max, x1_abs_max, (2 * x1_abs_max) / num_of_pixels)\n",
    "x2_vals = np.arange(x2_abs_max, -x2_abs_max, -(2 * x2_abs_max) / num_of_pixels)\n",
    "x2_vals.shape = (num_of_pixels, 1)\n",
    "\n",
    "x1_data = np.tile(x1_vals, (num_of_pixels, 1))\n",
    "x2_data = np.tile(x2_vals, (1, num_of_pixels))\n",
    "\n",
    "# Try changing the first parameter to lower values\n",
    "polar_map = polar_transform(2.0, [x1_data, x2_data])\n",
    "\n",
    "grid_range = np.arange(0, num_of_pixels + 25, 25)\n",
    "fig, ax = prepare_plot(grid_range, grid_range, figsize=(9.0, 7.2), hide_labels=True)\n",
    "image = plt.imshow(polar_map, interpolation='nearest', aspect='auto')\n",
    "ax.set_xlabel('Principal component one'), ax.set_ylabel('Principal component two')\n",
    "grid_marks = (2 * grid_range / float(num_of_pixels) - 1.0)\n",
    "x1_marks = x1_abs_max * grid_marks\n",
    "x2_marks = -x2_abs_max * grid_marks\n",
    "ax.get_xaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x1_marks))\n",
    "ax.get_yaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x2_marks))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same transformation on the image data\n",
    "# Try changing the first parameter to lower values\n",
    "brainmap = polar_transform(2.0, [image_one_scaled, image_two_scaled])\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature-based aggregation and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Aggregation using arrays\n",
    "\n",
    "In the analysis in Part 3, we performed PCA on the full time series data, trying to find global patterns across all 240 seconds of the time series. However, our analysis doesn't use the fact that different events happened during those 240 seconds. Specifically, during those 240 seconds, the zebrafish was presented with 12 different direction-specific visual patterns, with each one lasting for 20 seconds, for a total of 12 x 20 = 240 features. Stronger patterns are likely to emerge if we incorporate knowledge of our experimental setup into our analysis.  As we'll see, we can isolate the impact of temporal response or direction-specific impact by appropriately aggregating our features.\n",
    "\n",
    "In order to aggregate the features we will use basic ideas from matrix multiplication.  First, note that if we use `np.dot` with a two-dimensional array, then NumPy performs the equivalent matrix-multiply calculation.  For example, `np.array([[1, 2, 3], [4, 5, 6]]).dot(np.array([2, 0, 1]))` produces `np.array([5, 14])`.\n",
    "\n",
    "\\\\[\\begin{bmatrix} 1 & 2 & 3 \\\\\\ 4 & 5 & 6 \\end{bmatrix} \\begin{bmatrix} 2 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 14 \\end{bmatrix} \\\\]\n",
    "\n",
    "By setting up our multi-dimensional array properly we can multiply it by a vector to perform certain aggregation operations.  For example, imagine we had a 3 dimensional vector, \\\\( \\scriptsize \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}^\\top \\\\)  and we wanted to create a 2 dimensional vector containing the sum of its first and last elements as one value and three times its second value as another value, i.e., \\\\( \\scriptsize \\begin{bmatrix} 4 & 6 \\end{bmatrix}^\\top \\\\). We can generate this result via matrix multiplication as follows: `np.array([[1, 0, 1], [0, 3, 0]]).dot(np.array([1, 2, 3])` which produces `np.array([4, 6]`.\n",
    "\n",
    "\\\\[\\begin{bmatrix} 1 & 0 & 1 \\\\\\ 0 & 3 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\\\ 2 \\\\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 6 \\end{bmatrix} \\\\]\n",
    "\n",
    "For this exercise, you'll create several arrays that perform different types of aggregation.  The aggregation is specified in the comments before each array.  You should fill in the array values by hand.  We'll automate array creation in the next two exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1177249d21dd0250336dba7dc3b01406",
     "grade": false,
     "grade_id": "answer_aggregationArray4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to create arrays for different types of arregation\n",
    "vector = np.array([0., 1., 2., 3., 4., 5.])\n",
    "\n",
    "# # Create a multi-dimensional array that when multiplied (using .dot) against vector, results in\n",
    "# # a two element array where the first element is the sum of the 0, 2, and 4 indexed elements of\n",
    "# # vector and the second element is the sum of the 1, 3, and 5 indexed elements of vector.\n",
    "# # This should be a 2 row by 6 column array\n",
    "\n",
    "# sum_every_other = np.array(<FILL_IN)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Create a multi-dimensional array that when multiplied (using .dot) against vector, results in a\n",
    "# # three element array where the first element is the sum of the 0 and 3 indexed elements of vector,\n",
    "# # the second element is the sum of the 1 and 4 indexed elements of vector, and the third element is\n",
    "# # the sum of the 2 and 5 indexed elements of vector.\n",
    "# # This should be a 3 row by 6 column array\n",
    "\n",
    "# sum_every_third = np.array(<FILL_IN)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Create a multi-dimensional array that can be used to sum the first three elements of vector and\n",
    "# # the last three elements of vector, which returns a two element array with those values when dotted\n",
    "# # with vector.\n",
    "# # This should be a 2 row by 6 column array\n",
    "\n",
    "# sum_by_three = np.array(<FILL_IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Create a multi-dimensional array that that sums the first two elements, second two elements, and\n",
    "# # last two elements of vector, which returns a three element array with those values when dotted\n",
    "# # with vector.\n",
    "# # This should be a 3 row by 6 column array\n",
    "# sum_by_two = np.array(<FILL_IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('sum_every_other.dot(vector):\\t{0}'.format(sum_every_other.dot(vector)))\n",
    "print ('sum_every_third.dot(vector):\\t{0}'.format(sum_every_third.dot(vector)))\n",
    "\n",
    "print ('\\nsum_by_three.dot(vector):\\t{0}'.format(sum_by_three.dot(vector)))\n",
    "print ('sum_by_two.dot(vector): \\t{0}'.format(sum_by_two.dot(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9c20b2d3e960338c20d72e9de905b33",
     "grade": true,
     "grade_id": "test_aggregationArray4a",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Aggregation using arrays (4a)\n",
    "assert_equal(sum_every_other.shape, (2, 6), 'incorrect shape for sum_every_other')\n",
    "assert_equal(sum_every_third.shape, (3, 6), 'incorrect shape for sum_every_third')\n",
    "assert_true(np.allclose(sum_every_other.dot(vector), [6, 9]), 'incorrect value for sum_every_other')\n",
    "assert_true(np.allclose(sum_every_third.dot(vector), [3, 5, 7]),\n",
    "                'incorrect value for sum_every_third')\n",
    "assert_equal(sum_by_three.shape, (2, 6), 'incorrect shape for sum_by_three')\n",
    "assert_equal(sum_by_two.shape, (3, 6), 'incorrect shape for sum_by_two')\n",
    "assert_true(np.allclose(sum_by_three.dot(vector), [3, 12]), 'incorrect value for sum_by_three')\n",
    "assert_true(np.allclose(sum_by_two.dot(vector), [1, 5, 9]), 'incorrect value for sum_by_two')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Recreate with `np.tile` and `np.eye`\n",
    "[np.tile](http://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html) is useful for repeating arrays in one or more dimensions.  For example, `np.tile(np.array([[1, 2], [3, 4]]), 2)` produces `np.array([[1, 2, 1, 2], [3, 4, 3, 4]]))`.\n",
    "\n",
    " \\\\[ np.tile( \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} , 2) \\to \\begin{bmatrix} 1 & 2 & 1& 2 \\\\\\ 3 & 4 & 3 & 4 \\end{bmatrix} \\\\]\n",
    "\n",
    "Recall that [np.eye](http://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html) can be used to create an identity array \\\\( (\\mathbf{I_n}) \\\\).  For example, `np.eye(3)` produces `np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])`.\n",
    "\n",
    "\\\\[ np.eye( 3 ) \\to \\begin{bmatrix} 1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1 \\end{bmatrix} \\\\]\n",
    "\n",
    "In this exercise, recreate `sum_every_other` and `sum_every_third` using `np.tile` and `np.eye`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for what to recreate\n",
    "print ('sum_every_other: \\n{0}'.format(sum_every_other))\n",
    "print ('\\nsum_every_third: \\n{0}'.format(sum_every_third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a91914f333a4db8df6cc9590ea1099c9",
     "grade": false,
     "grade_id": "answer_tileAggregation4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to use np.tile and np.eye to recreate the arrays\n",
    "\n",
    "# sum_every_other_tile = <FILL_IN>\n",
    "# sum_every_third_tile = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print (sum_every_other_tile)\n",
    "print ('sum_every_other_tile.dot(vector): {0}'.format(sum_every_other_tile.dot(vector)))\n",
    "print ('\\n', sum_every_third_tile)\n",
    "print ('sum_every_third_tile.dot(vector): {0}'.format(sum_every_third_tile.dot(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134b35d6bfa5396889beafa1c14a8403",
     "grade": true,
     "grade_id": "test_tileAggregation4b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Recreate with `np.tile` and `np.eye` (4b)\n",
    "assert_equal(sum_every_other_tile.shape, (2, 6), 'incorrect shape for sum_every_other_tile')\n",
    "assert_equal(sum_every_third_tile.shape, (3, 6), 'incorrect shape for sum_every_third_tile')\n",
    "assert_true(np.allclose(sum_every_other_tile.dot(vector), [6, 9]),\n",
    "                'incorrect value for sum_every_other_tile')\n",
    "assert_true(np.allclose(sum_every_third_tile.dot(vector), [3, 5, 7]),\n",
    "                'incorrect value for sum_every_third_tile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Recreate with `np.kron`\n",
    "The Kronecker product is the generalization of outer products involving matrices, and we've included some examples below to illustrate the idea.  Please refer to the [Wikipedia page](https://en.wikipedia.org/wiki/Kronecker_product) for a detailed definition.  We can use [np.kron](http://docs.scipy.org/doc/numpy/reference/generated/numpy.kron.html) to compute Kronecker products and recreate the `sum_by` arrays.  Note that \\\\( \\otimes \\\\) indicates a Kronecker product.\n",
    "\n",
    "\\\\[ \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 & 2 \\end{bmatrix}  = \\begin{bmatrix} 1 \\cdot 1 & 1 \\cdot 2 & 2 \\cdot 1 & 2 \\cdot 2 \\\\\\ 3 \\cdot 1 & 3 \\cdot 2 & 4 \\cdot 1 & 4 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 2 & 4 \\\\\\ 3 & 6 & 4 & 8 \\end{bmatrix}  \\\\]\n",
    "\n",
    "We can see how the Kronecker product continues to expand if there are multiple rows in the second array.\n",
    "\n",
    "\\\\[ \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} & 2 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} \\\\\\ \\\\\\ 3 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} & 4 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 5 & 6 & 10 & 12 \\\\\\ 7 & 8 & 14 & 16 \\\\\\ 15 & 18 & 20 & 24 \\\\\\ 21 & 24 & 28 & 32 \\end{bmatrix} \\\\]\n",
    "\n",
    "For this exercise, you'll recreate the `sum_by_three` and `sum_by_two` arrays using `np.kron`, `np.eye`, and `np.ones`.  Note that `np.ones` creates an array of all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for what to recreate\n",
    "print ('sum_by_three: \\n{0}'.format(sum_by_three))\n",
    "print ('\\nsum_by_two: \\n{0}'.format(sum_by_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51d671ffe4b0dc57ebd9da867b845a99",
     "grade": false,
     "grade_id": "answer_kronAggregation4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to use np.kron, np.eye, and np.ones to recreate the arrays\n",
    "\n",
    "# sum_by_three_kron = <FILL IN>\n",
    "# sum_by_two_kron = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print (sum_by_three_kron)\n",
    "print ('sum_by_three_kron.dot(vector): {0}'.format(sum_by_three_kron.dot(vector)))\n",
    "print ('\\n', sum_by_two_kron)\n",
    "print ('sum_by_two_kron.dot(vector): {0}'.format(sum_by_two_kron.dot(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687f2d563c3d7ed51bc8666eb9fb0258",
     "grade": true,
     "grade_id": "test_kronAggregation4c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Recreate with `np.kron` (4c)\n",
    "assert_equal(sum_by_three_kron.shape, (2, 6), 'incorrect shape for sum_by_three_kron')\n",
    "assert_equal(sum_by_two_kron.shape, (3, 6), 'incorrect shape for sum_by_two_kron')\n",
    "assert_true(np.allclose(sum_by_three_kron.dot(vector), [3, 12]),\n",
    "                'incorrect value for sum_by_three_kron')\n",
    "assert_true(np.allclose(sum_by_two_kron.dot(vector), [1, 5, 9]),\n",
    "                'incorrect value for sum_by_two_kron')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Aggregate by time\n",
    "\n",
    "As we discussed in Part (4a), we would like to incorporate knowledge of our experimental setup into our analysis. To do this, we'll first study the temporal aspects of neural response, by aggregating our features by time. In other words, we want to see how different pixels (and the underlying neurons captured in these pixels) react in each of the 20 seconds after a new visual pattern is displayed, regardless of what the pattern is.  Hence, instead of working with the 240 features individually, we'll aggregate the original features into 20 new features, where the first new feature captures the pixel response one second after a visual pattern appears, the second new feature is the response after two seconds, and so on.\n",
    "\n",
    "We can perform this aggregation using a map operation. First, build a multi-dimensional array \\\\( \\scriptsize \\mathbf{T} \\\\) that, when dotted with a 240-component vector, sums every 20-th component of this vector and returns a 20-component vector. Note that this exercise is similar to (4b).  Once you have created your multi-dimensional array \\\\( \\scriptsize \\mathbf{T} \\\\), use a `map` operation with that array and each time series to generate a transformed dataset. We'll cache and count the output, as we'll be using it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3a3ef4ae5b68d0b58859ee320b8dfa6",
     "grade": false,
     "grade_id": "answer_timeAggregation4d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to create a multi-dimensional array to perform the aggregation\n",
    "# # Create a multi-dimensional array to perform the aggregation\n",
    "# T = <FILL IN>\n",
    "\n",
    "# # Transform scaled_data using T.  Make sure to retain the keys.\n",
    "# time_data = scaled_data.<FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "time_data.cache()\n",
    "print (time_data.count())\n",
    "print (time_data.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54731f17f417f23c3b0450a88d38d34e",
     "grade": true,
     "grade_id": "test_timeAggregation4d",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Aggregate by time (4d)\n",
    "assert_equal(T.shape, (20, 240), 'incorrect shape for T')\n",
    "time_data_first = time_data.values().first()\n",
    "time_data_fifth = time_data.values().take(5)[4]\n",
    "assert_equal(time_data.count(), 46460, 'incorrect length of time_data')\n",
    "assert_equal(time_data_first.size, 20, 'incorrect value length of time_data')\n",
    "assert_equal(time_data.keys().first(), (0, 0), 'incorrect keys in time_data')\n",
    "assert_true(np.allclose(time_data_first[:2], [0.00802155, 0.00607693], atol=1e-2),\n",
    "                'incorrect values in time_data')\n",
    "assert_true(np.allclose(time_data_fifth[-2:], [-0.00636676, -0.0179427], atol=1e-2),\n",
    "                'incorrect values in time_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Obtain a compact representation\n",
    "\n",
    "We now have a time-aggregated dataset with \\\\(\\scriptsize n = 46460\\\\) pixels and \\\\(\\scriptsize d = 20\\\\) aggregated time features, and we want to use PCA to find a more compact representation.  Use the `pca` function from Part (2a) to perform PCA on the this data with \\\\(\\scriptsize k = 3\\\\), resulting in a new low-dimensional 46,460 by 3 dataset. As before, you'll need to extract the values from `time_data` since it is an RDD of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5bca06dca3fcab761d3010653b05314",
     "grade": false,
     "grade_id": "answer_compactRep4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to extract a compact representation from time_data using pca function from Part (2a)\n",
    "\n",
    "# components_time, time_scores, eigenvalues_time = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('components_time: (first five) \\n{0}'.format(components_time[:5, :]))\n",
    "print ('\\ntime_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, time_scores.take(3)))))\n",
    "print ('\\neigenvalues_time: (first five) \\n{0}'.format(eigenvalues_time[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "743000470c210807f464c423eb1a8b9d",
     "grade": true,
     "grade_id": "test_compactRep4e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Obtain a compact representation (4e)\n",
    "assert_equal(components_time.shape, (20, 3), 'incorrect shape for components_time')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_time[:5, :])), 2.37299020, atol=1e-2),\n",
    "                'incorrect value for components_time')\n",
    "assert_true(np.allclose(np.abs(np.sum(time_scores.take(3))), 0.0213119114, atol=1e-2),\n",
    "                'incorrect value for time_scores')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_time[:5]), 0.844764792, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 9: Top two components by time\n",
    "\n",
    "Let's view the scores from the first two principal components as a composite image. When we preprocess by aggregating by time and then perform PCA, we are only looking at variability related to temporal dynamics. As a result, if neurons appear similar -- have similar colors -- in the resulting image, it means that their responses vary similarly over time, regardless of how they might be encoding direction. In the image below, we can define the midline as the horizontal line across the middle of the brain.  We see clear patterns of neural activity in different parts of the brain, and crucially note that the regions on either side of the midline are similar, which suggests that temporal dynamics do not differ across the two sides of the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_time = np.vstack(time_scores.collect())\n",
    "image_one_time = scores_time[:, 0].reshape(230, 202).T\n",
    "image_two_time = scores_time[:, 1].reshape(230, 202).T\n",
    "brainmap = polar_transform(3, [image_one_time, image_two_time])\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Aggregate by direction\n",
    "\n",
    "Next, let's perform a second type of feature aggregation so that we can study the direction-specific aspects of neural response, by aggregating our features by direction. In other words, we want to see how different pixels (and the underlying neurons captured in these pixels) react when the zebrafish is presented with 12 direction-specific patterns, ignoring the temporal aspect of the reaction.  Hence, instead of working with the 240 features individually, we'll aggregate the original features into 12 new features, where the first new feature captures the average pixel response to the first direction-specific visual pattern, the second new feature is the response to the second direction-specific visual pattern, and so on.\n",
    "\n",
    "As in Part (4c), we'll design a multi-dimensional array \\\\( \\scriptsize \\mathbf{D} \\\\) that, when multiplied by a 240-dimensional vector, sums the first 20 components, then the second 20 components, and so on. Note that this is similar to exercise (4c).  First create \\\\( \\scriptsize \\mathbf{D} \\\\), then use a `map` operation with that array and each time series to generate a transformed dataset. We'll cache and count the output, as we'll be using it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67f9e0eccd8886dcacf3d49627fc4ef9",
     "grade": false,
     "grade_id": "answer_directionAggregation4f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to create a multi-dimensional array to perform the aggregation\n",
    "# D = <FILL IN>\n",
    "\n",
    "# # Transform scaled_data using D.  Make sure to retain the keys.\n",
    "# direction_data = scaled_data. <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "direction_data.cache()\n",
    "print (direction_data.count())\n",
    "print (direction_data.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fb20fc7256e7df50ad0fd71803b1d81",
     "grade": true,
     "grade_id": "test_directionAggregation4f",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Aggregate by direction (4f)\n",
    "assert_equal(D.shape, (12, 240), 'incorrect shape for D')\n",
    "direction_data_first = direction_data.values().first()\n",
    "direction_data_fifth = direction_data.values().take(5)[4]\n",
    "assert_equal(direction_data.count(), 46460, 'incorrect length of direction_data')\n",
    "assert_equal(direction_data_first.size, 12, 'incorrect value length of direction_data')\n",
    "assert_equal(direction_data.keys().first(), (0, 0), 'incorrect keys in direction_data')\n",
    "assert_true(np.allclose(direction_data_first[:2], [0.03346365, 0.03638058], atol=1e-2),\n",
    "                'incorrect values in direction_data')\n",
    "assert_true(np.allclose(direction_data_fifth[:2], [0.01479147, -0.02090099], atol=1e-2),\n",
    "                'incorrect values in direction_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4g) Compact representation of direction data\n",
    "\n",
    "We now have a direction-aggregated dataset with \\\\(\\scriptsize n = 46460\\\\) pixels and \\\\(\\scriptsize d = 12\\\\) aggregated direction features, and we want to use PCA to find a more compact representation.  Use the `pca` function from Part (2a) to perform PCA on the this data with \\\\(\\scriptsize k = 3\\\\), resulting in a new low-dimensional 46460 by 3 dataset. As before, you'll need to extract the values from `direction_data` since it is an RDD of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "827059dee1bf5d198a8f0e8b0673be3c",
     "grade": false,
     "grade_id": "answer_compactDirection4g",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to get compact representation of direction data using pca function\n",
    "# components_direction, direction_scores, eigenvalues_direction = <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('components_direction: (first five) \\n{0}'.format(components_direction[:5, :]))\n",
    "print ('\\ndirection_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, direction_scores.take(3)))))\n",
    "print ('\\neigenvalues_direction: (first five) \\n{0}'.format(eigenvalues_direction[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a06564551c93ddbaed12b66c58d578e",
     "grade": true,
     "grade_id": "test_compactDirection4g",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Compact representation of direction data (4g)\n",
    "assert_equal(components_direction.shape, (12, 3), 'incorrect shape for components_direction')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_direction[:5, :])), 1.080232069, atol=1e-2),\n",
    "                'incorrect value for components_direction')\n",
    "assert_true(np.allclose(np.abs(np.sum(direction_scores.take(3))), 0.10993162084, atol=1e-2),\n",
    "                'incorrect value for direction_scores')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_direction[:5]), 2.0089720377, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_direction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 10: Top two components by direction\n",
    "\n",
    "Again, let's view the scores from the first two principal components as a composite image.  When we preprocess by averaging across time (group by direction), and then perform PCA, we are only looking at variability related to stimulus direction. As a result, if neurons appear similar -- have similar colors -- in the image, it means that their responses vary similarly across directions, regardless of how they evolve over time. In the image below, we see a different pattern of similarity across regions of the brain.  Moreover, regions on either side of the midline are colored differently, which suggests that we are looking at a property, direction selectivity, that has a different representation across the two sides of the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_direction = np.vstack(direction_scores.collect())\n",
    "image_one_direction = scores_direction[:, 0].reshape(230, 202).T\n",
    "image_two_direction = scores_direction[:, 1].reshape(230, 202).T\n",
    "brainmap = polar_transform(2, [image_one_direction, image_two_direction])\n",
    "# with thunder: Colorize(cmap='polar', scale=2).transform([image_one_direction, image_two_direction])\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap, interpolation='nearest', aspect='auto')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "CTR_DF_answers",
  "notebookId": 743615222353704
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
