{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Machine Learning with Large Datasets\n",
    "## Homework 4 - Machine Learning at Scale: Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with this notebook, make sure you have already completed the data conversion step on AWS.\n",
    "\n",
    "Note that we will not be autograding this notebook because of the open-ended nature of it (although you will have to submit this notebook). To make grading easier and to learn about your thought process, throughout the notebook, we include questions you have to anwswer in your writeup. We have indicated locations in the notebook corresponding to these questions with a ✰ symbol.\n",
    "\n",
    "### 0. Start a Spark Session and Install Libraries\n",
    "\n",
    "As a first step, you should \n",
    "\n",
    "- start a Spark session on your cluster, and \n",
    "\n",
    "- check how many executor instances you have and whether that matches your configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this assignment, you will be generating plots. `Matplotlib` and other useful Python libraries do not come pre-installed on the cluster. Therefore, you will have to ssh into your master node (think about why it should be the master) using your keypair created earlier and install `matplotlib`. You might have to do this later again for other libraries you use.\n",
    "\n",
    "Run the cell below to ensure you installation was successful. If an error occurs, you might want to double check your installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Preparation\n",
    "\n",
    "Earlier, we have extracted relevant features from and converted format of the full raw Million Song Dataset. We now want to load our converted dataset from the S3 Storage.\n",
    "\n",
    "Use something like this: \n",
    "\n",
    "```\n",
    "df = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"s3://<bucket_name>/<path>/<file_name>.csv\")\n",
    "```\n",
    "\n",
    "Note that although you can load all chunks of the dataset using `*`, we recommend you only load in a subset while developing so that processing takes shorter time when you are just verifying your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we inspect the `df` we just created by running the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a few problems:\n",
    "\n",
    "- Because we did not include headers in the CSV files, Spark does not know the name of the features, and hence the \"_c0\", \"_c1\", ... that we see\n",
    "- Although we set `inferSchema=True` when loading data, all array types were still interpreted as plain strings.\n",
    "\n",
    "Let's first recover all the names of the features. You could reuse the feature name array you used in your `million_song_reader.py` from the conversion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we run the below cell again, we should see proper feature names being attached to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are still a few features, e.g. `artist_latitude`, not being converted to the correct type. Let's do this manually and convert numeric features to `pyspark.sql.types.DoubleType` (Hint: there should be 19 of them). ✰ List the 19 numeric features in your writeup.\n",
    "\n",
    "Don't worry about array features for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set for now. Let's run the following cell to inspect everything except the arrays looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to grade your checkpoint, run the following cell and ✰ include the output in your writeup.\n",
    "\n",
    "Some sanity checks based on our reference solution:\n",
    "- There should be 19 numeric features\n",
    "- There should be around 580k data records\n",
    "- `song_hotttnesss` should be a floating point number between 0 and 1, with mean around 0.36\n",
    "- `artist_name` and `title` should be human-readable text, rather than undecoded bytes\n",
    "- `artist_terms` should be a string literal of an array containing human-readable tags, rather than undecoded bytes\n",
    "- The max of `year` should be 2011 (because MSD was published in 2011)\n",
    "\n",
    "We will have some wiggle rooms in grading because everyone might have processed the data slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cols = [t for t in df.dtypes if t[1]=='double']\n",
    "str_cols = [t for t in df.dtypes if t[1] == 'string']\n",
    "print('total feature {}, numeric feature {}, string feature {}'.format(len(df.dtypes),len(double_cols),len(str_cols)))\n",
    "print('total {} records'.format(df.count()))\n",
    "print('\\nsample data record:')\n",
    "head = df.head()\n",
    "features = ['song_hotttnesss', 'artist_hotttnesss', 'artist_id', 'artist_latitude', 'artist_name',\n",
    "           'title', 'danceability', 'duration', 'loudness', 'year', 'artist_terms', 'artist_terms_freq']\n",
    "for f in features:\n",
    "    print(f'  {f}: {head[f]}')\n",
    "print()\n",
    "df.select('song_hotttnesss', 'artist_hotttnesss', 'year').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Part A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
