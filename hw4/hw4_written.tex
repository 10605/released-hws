\documentclass{exam}

%------------------------ packages ------------------------%
\usepackage{amsmath,amsfonts,amsthm,amssymb,amsopn,bm}
\usepackage{pythontex}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[paper=letterpaper,margin=1in,includeheadfoot,footskip=0.25in,headsep=0.25in]{geometry}
\usepackage{url}
\usepackage[usenames,dvipsnames]{color}
\usepackage[pdfborder={0 0 1},colorlinks=true,citecolor=black,plainpages=false]{hyperref}
%\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage{pdfpages,bbm}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}


%------------------------ math ------------------------%
\newcommand{\R}{\mathbb{R}} % real domain
\newcommand{\Rset}{\mathbb{R}} % real domain
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\av}{\mathbf{a}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\Proj}{\mathbf{P}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\U}{\mathbf{U}}
\renewcommand{\L}{\mathbf{L}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\mD}{\mathbf{D}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\rank}{rank}
\newcommand{\E}{\mathbb{E}}


%------------------------ exam class macros ------------------------%
\checkboxchar{$\Box$}
% \renewcommand{\questionshook}{%
%     \setlength{\leftmargin}{10pt}%
%     \setlength{\labelwidth}{-\labelsep}%
% }
\renewcommand{\checkboxeshook}{
  \settowidth{\leftmargin}{W.}
  \labelwidth\leftmargin\advance\labelwidth-\labelsep
}

\newcommand{\grade}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}


\begin{document}

\title{Homework 4 Written Assignment}
\author{\Large \bf 10-605/10-805: Machine Learning with Large Datasets}
\date{{\bf Due Thursday, November 5th at 1:30:00 PM Eastern Time}}
\maketitle

\paragraph{Instructions:} Submit your solutions via Gradescope, \textit{with your solution to each subproblem on a separate page}, i.e., following the template below.  Note that Homework 4 consists of two parts: this written assignment, and a programming assignment. The written part is worth \textbf{50\%} of your total HW4 grade. The programming part makes up the remaining 50\%.

\paragraph{Submitting via Gradescope:} When submitting on Gradescope, you must assign pages to each question correctly (it
prompts you to do this after submitting your work). This significantly streamlines the
grading process for the course staff.  Failure to do this may result in a score of 0 for any questions
that you didn't correctly assign pages to. It is also your responsibility to make sure that your scan/submission is legible so that we can grade it.

\paragraph{Collaboration:}  While you may talk with others about the homework, you must each turn in your own solution and it should be your own work. \text{Please list the names of any collaborators below.}

\paragraph{Collaborators:}


\newpage

\section{Autodiff: Simple Neural Network (24 pts)}
Consider a feedforward neural network, defined by the following composition of functions:
\begin{align}
    \bold{q} &= W^{(1)}x + \bold{b^{(1)}} \\
    \bold{h} &= ReLU(\bold{q}) = \max(\bold{q}, 0) \\
    \bold{p} &= W^{(2)}\bold{h} + \bold{b^{(2)}} \\ 
    \bold{L}(\bold{y}, \bold{p}) &= (\bold{p} - \bold{y})^T(\bold{p} - \bold{y}) 
\end{align}


\noindent Here $x \in \mathbb{R}^D$ is an input observation/data point, $W^{(1)} \in \mathbb{R}^{H^{(1)} \times D}$ is a set of weights corresponding to the first (input) layer, $\bold{b^{(1)}} \in \mathbb{R}^{H^{(1)}}$ is an offset term corresponding to the first layer, $W^{(2)} \in \mathbb{R}^{H^{(2)} \times H^{(1)}}$ is a set of weights corresponding to the second layer, and $\bold{b}^{(2)} \in \mathbb{R}^{H^{(2)}}$ is an offset term corresponding to the second layer. Note also that the max in step 2 is applied element-wise. 
\\
\\
Our goal is to find the gradient of the loss, $L$, with respect to the weights $W$ and $b$ using backpropogation. We will first explore backpropagation on the simple neural network above, and then a more general (``deep'') network in Problem 2. Note that you may need the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta}: 
$$\delta_{ij} = \begin{cases} 0 & i \neq j \\ 1 & i = j \end{cases}$$

\begin{enumerate}[label=(\alph*)]
    \item\grade{10} First derive the following five terms: $$\frac{\partial L}{\partial p_j}, \, \frac{\partial p_i}{\partial W^{(2)}_{j,k}}, \, \frac{\partial L}{\partial W^{(2)}_{j, k}}, \, \frac{\partial L}{\partial b^{(2)}_j}, \, \frac{\partial p_i}{\partial h_j}$$
    

    \newpage
    \item \grade{6} Now derive the following three terms: $$\frac{\partial h_j}{\partial q_i}, \, \frac{\partial L}{\partial h_i}, \, \frac{\partial L}{\partial q_i}$$
    

    
    \newpage
    \item \grade{8} Finally, derive the following four terms, which are what we set out to calculate: $$\frac{\partial q_i}{\partial W^{(1)}_{j, k}}, \, \frac{\partial q_i}{\partial b^{(1)}_j}, \, \frac{\partial L}{\partial W^{(1)}_{j, k}}, \, \frac{\partial L}{\partial b^{(1)}_{i}}$$
    
\end{enumerate}
\newpage

\section{Autodiff: Deep Learning (26 pts)}
Now, let's generalize Problem 1 to a larger neural network. Suppose that we create a deep network by using $L$ fully connected hidden layers with ReLU activations, and we use the squared loss for the final layer (i.e., similar to the network in Problem 1, but with $L$ hidden layers).
\begin{enumerate}[label=(\alph*)]
\item \grade{8}  Write down the expression for inference for this neural network (i.e., the forward pass). (Hint: you may want to use a recurrence relation, similar to what we saw in lecture.)

\newpage
\item \grade{5} Suppose that the sizes of the layers are $H^{(0)} = H^{(1)} = \dots = H^{(L)} = 1$. What is the computational cost of running inference on the deep neural network described in part (a)? In other words, how many numerical
operations would be required to compute the forward pass? Which operation dominates: the matrix
multiplies or the nonlinearities?

\newpage
\item \grade{8} Now write down the expression for backpropagation on this deep neural network. That is, how would you compute the gradient of the network with
respect to the weights for a training example x? (Hint: This should look very similar to what you have in Problem 1).

\newpage
\item \grade{5} What is the computational cost of running backpropagation on this deep neural network? In other words, how many numerical operations would be required to perform backpropagation? Which operation
dominates: the matrix multiplies or the nonlinearities?

\end{enumerate}


\end{document}