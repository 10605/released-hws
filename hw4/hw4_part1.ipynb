{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Create Assignment",
    "colab": {
      "name": "hw4_part1_student.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7AVMloGX_gz"
      },
      "source": [
        "# CMU 10-405/10-605: Machine Learning with Large Datasets auto-graded notebook\n",
        "\n",
        "Before you turn this assignment in, make sure everything runs as expected. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDyGj8DmXCJI"
      },
      "source": [
        "In this assignment, we will implement the style transfer application described in the paper [Leon A. Gatys' paper, A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576). \n",
        "\n",
        "### Five major steps:\n",
        "1. Visualize the data and expected outputs\n",
        "2. Process the data\n",
        "3. Create the model\n",
        "4. Define the loss function \n",
        "5. Optimize for the loss function and evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ajP_u73s6m"
      },
      "source": [
        "# Import Packages and Supporting Files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "# Import packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.python.keras.preprocessing import image as k_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYEjlrYk3s6w"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import image as kp_image\n",
        "from tensorflow.python.keras import models \n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixla0iaIWKVg"
      },
      "source": [
        "#Download the Content Image and Style Image\n",
        "!wget 'https://upload.wikimedia.org/wikipedia/commons/c/cd/VanGogh-starry_night.jpg' -O style.jpg\n",
        "!wget 'https://upload.wikimedia.org/wikipedia/commons/e/e6/T%C3%BCbingen_Neckarfront_August_2013.jpg' -O content.jpg\n",
        "!wget 'https://raw.githubusercontent.com/10605/data/master/hw4/styleTransfer/my_styled_image.jpg' -O style_output.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOiGrIV1iERH"
      },
      "source": [
        "# Set path variables here\n",
        "content_path = 'content.jpg'\n",
        "style_path = 'style.jpg'\n",
        "styled_ref='style_output.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "# Visualize the Inputs and the Reference Stylized Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def resize_img(path_to_img):\n",
        "    max_dim = 512\n",
        "    img = Image.open(path_to_img)\n",
        "    long = max(img.size)\n",
        "    scale = max_dim/long\n",
        "    img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
        "    img = kp_image.img_to_array(img)\n",
        "  \n",
        "    # We need to broadcast the image array such that it has a batch dimension \n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vupl0CI18aAG"
      },
      "source": [
        "def imshow(img, title=None):\n",
        "    # Remove the batch dimension\n",
        "    out = np.squeeze(img, axis=0)\n",
        "    # Normalize for display \n",
        "    out = out.astype('uint8')\n",
        "    plt.imshow(out)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.imshow(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWQmeEaiKkP"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "content = resize_img(content_path).astype('uint8')\n",
        "style = resize_img(style_path).astype('uint8')\n",
        "ref_output=resize_img(styled_ref).astype('uint8')\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content, 'Content Image')\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style, 'Style Image')\n",
        "plt.show()\n",
        "imshow(ref_output,'Reference Output after Style Transfer')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qMVNvEsK-_D"
      },
      "source": [
        "# Prepare the Data\n",
        "Let's implement some helper functions that will allow us to load and preprocess our images more easily. We will be using VGG as the backbone model. It is worth noting that VGG is trained on images with each channel normalized by `mean = [103.939, 116.779, 123.68]` and with channels BGR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGwmTwJNmv2a",
        "nbgrader": {
          "grade": false,
          "grade_id": "load_and_process_img",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def load_and_process_img(path_to_img):\n",
        "    \"\"\"\n",
        "    Load and process the image at the given path\n",
        "\n",
        "    Note:\n",
        "      You should load the image using the resize_img function and use\n",
        "      keras.applications.vgg19.preprocess_input to change the image from\n",
        "      RGB to BGR format\n",
        "    \n",
        "    Args:\n",
        "      path_to_img (str): Path to the image\n",
        "    \n",
        "    Returns:\n",
        "      numpy.ndarray: The processed image\n",
        "    \"\"\"\n",
        "    \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # img = <FILL IN>\n",
        "    # # Preprocess them with respect to VGG19 stats\n",
        "    # img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "    return img\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57mIN1d2VoZV",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_load_and_process_img",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test load_and_process_img\n",
        "assert -10> np.mean(load_and_process_img(content_path))> -20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCgooqs6tAka"
      },
      "source": [
        "In order to visualize the output image of our model, we need to perform the inverse preprocessing step. Furthermore, since our output image may take its values anywhere between $- \\infty$ and $\\infty$, we must clip to maintain our values within the 0-255 range.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjzlKRQRs_y2",
        "nbgrader": {
          "grade": false,
          "grade_id": "deprocess_img",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def deprocess_img(processed_img):\n",
        "    \"\"\"\n",
        "    Perform inverse processing on the input image\n",
        "\n",
        "    Note: \n",
        "      Check if processed_img is 4 or 3 dimensional, raise expection if less than 3\n",
        "      else reduce the dimension to 3 by squeezing the appropriate axis and perform\n",
        "      inverse of preprocessing step. Remember that you need to add\n",
        "      mean = [103.939, 116.779, 123.68] with channels BGR.\n",
        "      Also for any image the pixel value should be clipped between 0-255.\n",
        "    \n",
        "    Args:\n",
        "      processed_img (numpy.ndarray): The processed image\n",
        "    \n",
        "    Returns:\n",
        "      numpy.ndarray: The inverse processed image\n",
        "    \"\"\"\n",
        "    \n",
        "    x = processed_img.copy()\n",
        "    if len(x.shape) == 4:\n",
        "        x = np.squeeze(x, 0)\n",
        "    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
        "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
        "    if len(x.shape) != 3:\n",
        "        raise ValueError(\"Invalid input to deprocessing image\")\n",
        "\n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # # add mean\n",
        "    # <FILL IN>\n",
        "    # # clip\n",
        "    # x = <FILL IN>\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNeylsVSw3lp",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_deprocess_img",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test deprocess_img\n",
        "content_img_test = load_and_process_img(content_path)\n",
        "d = deprocess_img(content_img_test)\n",
        "mean_d = np.mean(d)\n",
        "\n",
        "assert 200 > mean_d > 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3i3RRrJiOX"
      },
      "source": [
        "# Build the Model \n",
        "We will load [VGG19](https://keras.io/applications/#vgg19), and feed in our input tensor to the model to extract the feature maps of the content, style, and generated images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEwZ7FlwrjoZ"
      },
      "source": [
        "### Extract content and style representations from VGG\n",
        "For content layers we will use `conv2`  of `block5`.\n",
        "\n",
        "For style layers we will use `conv1` of each block. For example, `lock1_conv1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqvy86wHlMe2"
      },
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-8eUp_Kc-j",
        "nbgrader": {
          "grade": false,
          "grade_id": "content_layer",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "'''\n",
        "Define 4 variables:\n",
        "`content_layers` and `style_layers`: lists containing the names of the blocks to be used\n",
        "`num_content_layers` and `num_style_layers`: lengths of content_layers and style_layers\n",
        "'''\n",
        "\n",
        "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "\n",
        "# content_layers = <FILL IN>\n",
        "# \n",
        "# # Style layers we are interested in\n",
        "# style_layers = <FILL IN>\n",
        "# num_content_layers = <FILL IN>\n",
        "# num_style_layers = <FILL IN>\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S57jnuaRiUY-",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_content_layer",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#Test the content_layers \n",
        "assert style_layers[1]=='block2_conv1'\n",
        "assert num_content_layers==1\n",
        "assert num_style_layers==5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9AnzEUU6hhx"
      },
      "source": [
        "In order to access the intermediate layers corresponding to our style and content feature maps, we will get the corresponding outputs and use the Keras [**Functional API**](https://keras.io/getting-started/functional-api-guide/). We then define our model with the desired output activations. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfec6MuMAbPx",
        "nbgrader": {
          "grade": false,
          "grade_id": "model_vgg",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def model_VGG():\n",
        "    \"\"\" \n",
        "    Creates our model with access to intermediate layers. \n",
        "\n",
        "    Note:\n",
        "      This function will load the VGG19 model and access the intermediate layers. \n",
        "      These layers will then be used to create a new model that will take input image\n",
        "      and return the representations from these intermediate layers via VGG. \n",
        "  \n",
        "    Returns:\n",
        "        model: A keras model that takes image inputs and returns the style and \n",
        "        content intermediate layer outputs.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load a pretrained VGG on Imagenet\n",
        "    vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    \n",
        "    # Get output layers corresponding to style and content layers \n",
        "    style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
        "    \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # content_outputs = <FILL IN>\n",
        "    # model_outputs = style_outputs + content_outputs\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    return models.Model(vgg.input, model_outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJdYvJTZ4bdS"
      },
      "source": [
        "## Define the Loss function (content loss + style loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Hcepii7_qh"
      },
      "source": [
        "### Content Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FvH-gwXi4nq"
      },
      "source": [
        "Content loss \n",
        "\n",
        " $$L^l_{content}(p, x) = \\sum_{i, j} (F^l_{ij}(x) - P^l_{ij}(p))^2$$\n",
        "\n",
        "where F is the feature representation of the generated image and P is the feature representation\n",
        "of the content image layer l. The paper suggests that we use the feature map from the layer\n",
        "'conv4_2'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2mf7JwRMkCd",
        "nbgrader": {
          "grade": false,
          "grade_id": "content_loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def compute_content_loss(base_content, target):\n",
        "    \"\"\"\n",
        "    Calculate the content loss\n",
        "\n",
        "    Note:\n",
        "      You can use tf.reduce_mean\n",
        "    \n",
        "    Args:\n",
        "      base_content (numpy.ndarray): The generated image\n",
        "      target (numpy.ndarray): The content image\n",
        "    \n",
        "    Returns:\n",
        "      The content loss\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBJ626Js0_jH",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_content_loss",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test the Content Loss Function\n",
        "content_img=load_and_process_img(content_path)\n",
        "compute_content_loss(d,content_img).numpy()\n",
        "\n",
        "assert 30000>compute_content_loss(d,content_img).numpy()>1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGUfttK9F8d5"
      },
      "source": [
        "### Style Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6XtkGK_YGD1"
      },
      "source": [
        "Style loss of the base input image, $x$, and the style image, $a$, as the distance between the style representation (the gram matrices) of these images. We describe the style representation of an image as the correlation between different filter responses given by the Gram matrix  $G^l$, where $G^l_{ij}$ is the inner product between the vectorized feature map $i$ and $j$ in layer $l$. We can see that $G^l_{ij}$ generated over the feature map for a given image represents the correlation between feature maps $i$ and $j$. \n",
        "\n",
        "\n",
        "\n",
        "$$E_l =  \\sum_{i,j}(G^l_{ij} - A^l_{ij})^2$$\n",
        "\n",
        "where $G^l_{ij}$ and $A^l_{ij}$ are the respective style representation in layer $l$ of $x$ and $a$. $N_l$ describes the number of feature maps, each of size $M_l = height * width$. Thus, the total style loss across each layer is \n",
        "$$L_{style}(a, x) = \\sum_{l \\in L} w_l E_l$$\n",
        "where we weight the contribution of each layer's loss by giving more emphasis to\n",
        "deep layers. For example, w for ‘conv1_1’ can be 1, then weight for ‘conv2_2’ can be 2, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F21Hm61yLKk5"
      },
      "source": [
        "### Computing the style loss\n",
        "We will first implement a distance metric that will be used when calculating the style loss in `compute_style_loss()` below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jKdxnr82X9b",
        "nbgrader": {
          "grade": false,
          "grade_id": "gram_matrix",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "    \"\"\" \n",
        "    Create and return the gram matrix for `input_tensor`\n",
        "    \n",
        "    Note:\n",
        "      You'll first have to find the channels then reshape\n",
        "      the input_tensor, compute gram_matrix and divide it\n",
        "      by shape of input_tensor\n",
        "\n",
        "    Args:\n",
        "      input_tensor (tf.Tensor): The input image\n",
        "    \n",
        "    Returns:\n",
        "      tf.Tensor: The gram matrix\n",
        "    \"\"\"\n",
        "\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # a = tf.reshape(<FILL IN>)\n",
        "    # gram = <FILL IN>\n",
        "    # n = <FILL IN>\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "     \n",
        "    return gram / tf.cast(n, tf.float32)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkSH_gS361OE",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_gram_matrix",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test Gram Matrix\n",
        "sample_tensor_x=tf.constant([[1.,2.,4,2],[1.,2.,4,44.],[2.,2,2.,2],[12.,4.,1,8]])\n",
        "np_gram=np.mean(gram_matrix(sample_tensor_x).numpy())\n",
        "\n",
        "assert 200>np_gram>40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7MOqwKLLke8",
        "nbgrader": {
          "grade": false,
          "grade_id": "style_loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def compute_style_loss(base_style, gram_target):\n",
        "    \"\"\"\n",
        "    Compute the stype loss\n",
        "\n",
        "    Note:\n",
        "      Expects two images of dimension h, w, c: height, width, num filters of each layer\n",
        "\n",
        "    Args:\n",
        "      base_style (tf.Tensor): The base input image\n",
        "      gram_target (tf.Tensor): The target image\n",
        "    \n",
        "    Returns:\n",
        "      The style loss (G-A)**2\n",
        "    \"\"\"\n",
        "    \n",
        "    height, width, channels = base_style.get_shape().as_list()\n",
        "    \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # gram_style = <FILL IN>\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "    return tf.reduce_mean(tf.square(gram_style - gram_target)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YPo1uETftfT",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_style_loss",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test style Loss\n",
        "sample_tensor_y=tf.constant([[[1.,2.,4],[1.,2.,4],[2.,2,2],[12.,4.,1]],[[1.,2.,4],[1.,2.,4],[2.,2,2.],[12.,4.,1]],[[1.,2.,4],[1.,2.,4],[2.,2,2.],[12.,4.,1]],[[1.,2.,4],[1.,2.,4],[2.,2,2.],[12.,4.,1]]])\n",
        "sample_tensor_z=tf.constant([[1.,2.,4],[1.,2.,4],[2.,2,2]])\n",
        "test_style_output=compute_style_loss(sample_tensor_y, sample_tensor_z).numpy()\n",
        "assert 210>test_style_output>150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXIUX6czZABh"
      },
      "source": [
        "## Apply style transfer to our images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-lj5LxgtmnI",
        "nbgrader": {
          "grade": false,
          "grade_id": "compute_features",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def compute_features(model, content_path, style_path):\n",
        "    \"\"\"\n",
        "    Compute the content and style features\n",
        "  \n",
        "    Args:\n",
        "        model: The model that we are using.\n",
        "        content_path (str): The path to the content image.\n",
        "        style_path (str): The path to the style image\n",
        "    \n",
        "    Returns:\n",
        "        Tuple: The style features and the content features. \n",
        "    \"\"\"\n",
        "    \n",
        "    # Load our images in \n",
        "    content_image = load_and_process_img(content_path)\n",
        "    style_image = load_and_process_img(style_path)\n",
        "    \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    \n",
        "    # # batch compute content and style features\n",
        "    # style_outputs = model(style_image)\n",
        "    # content_outputs = <FILL IN>\n",
        "    # # Get the style and content feature representations from our model  \n",
        "    # style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
        "    # content_features = <FILL IN>\n",
        "  \n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    return style_features, content_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL-VsRvCmw9v",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_compute_features",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test compute_features\n",
        "test_style_features, test_content_features = compute_features(model_VGG() , content_path, style_path)\n",
        "test_style_len=len(test_style_features)\n",
        "test_content_len=len(test_content_features)\n",
        "test_style_mean=np.mean(test_style_features[0].numpy())\n",
        "test_content_mean=np.mean(test_content_features[0].numpy())\n",
        "assert test_style_len>=5\n",
        "assert test_content_len>=1\n",
        "assert 30>test_style_mean> 20\n",
        "assert 25>test_content_mean>15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DopXw7-lFHa"
      },
      "source": [
        "### Computing the loss and gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVDhSo8iJunf",
        "nbgrader": {
          "grade": false,
          "grade_id": "calculate_loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def calculate_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
        "    \"\"\"\n",
        "    Compute the total loss.\n",
        "  \n",
        "    Args:\n",
        "        model: The model that will give us access to the intermediate layers\n",
        "        loss_weights: The weights of each contribution of each loss function. \n",
        "                  (style weight, content weight)\n",
        "        init_image: the base image\n",
        "        gram_style_features: Precomputed gram matrices corresponding to the \n",
        "                        defined style layers\n",
        "        content_features: Precomputed outputs from defined content layers  \n",
        "      \n",
        "    Returns:\n",
        "        Tuple: total_loss, style_loss, content_loss\n",
        "    \"\"\"\n",
        "\n",
        "    style_weight, content_weight = loss_weights\n",
        "    \n",
        "    # Feed our init image through our model. This will give us the content and \n",
        "    # style representations at our desired layers. Since we're using eager\n",
        "    # our model is callable just like any other function!\n",
        "    model_outputs = model(init_image)\n",
        "  \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # style_output_features = <FILL IN>\n",
        "    # content_output_features = <FILL IN>\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "  \n",
        "    style_loss = 0\n",
        "    content_loss = 0\n",
        "\n",
        "    # Accumulate style losses from all layers\n",
        "    weight_per_style_layer = [1.0,2.0,3.0,4.0,5.0]\n",
        "    for target_style, comb_style, wg_layer in zip(gram_style_features, style_output_features,weight_per_style_layer):\n",
        "        # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "        # style_loss += <FILL IN>\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    # Accumulate content losses from all layers     \n",
        "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
        "    for target_content, comb_content in zip(content_features, content_output_features):\n",
        "        # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "        # content_loss += <FILL IN>\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "  \n",
        "    style_loss *= style_weight\n",
        "    content_loss *= content_weight\n",
        "    \n",
        "    \n",
        "    loss = style_loss + content_loss \n",
        "    return loss, style_loss, content_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktX7ozQivVLn",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_calculate_loss",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test calculate_loss\n",
        "init_image = tf.convert_to_tensor(load_and_process_img(content_path))\n",
        "loss_weights=(1000, 0.001)\n",
        "gram_style_features = [gram_matrix(style_feature) for style_feature in test_style_features]\n",
        "test_loss, test_style_loss, test_content_loss = calculate_loss(model_VGG(),\n",
        "                                                             loss_weights,\n",
        "                                                             init_image,\n",
        "                                                             gram_style_features,\n",
        "                                                             test_content_features)\n",
        "assert 1e16 > test_loss>1e3\n",
        "assert 1e16 > test_style_loss>1e3\n",
        "assert test_content_loss>=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9yKu2PLlBIE"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDQXM_5yAH4B"
      },
      "source": [
        "Now we will execute the training loop. Here we use [**tf.GradientTape**](https://www.tensorflow.org/programmers_guide/eager#computing_gradients) to compute the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj_enNo6tACQ",
        "nbgrader": {
          "grade": false,
          "grade_id": "run_style_transfer",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "import IPython.display\n",
        "def run_style_transfer(content_path, style_path, num_iterations): \n",
        "    '''\n",
        "    Execute the training loop\n",
        "\n",
        "    Note:\n",
        "      Major steps:\n",
        "        1. Load the VGG model and set layers trainable to False.\n",
        "          Since we don't need to  train any layers of our model, \n",
        "          so we set trainable to false\n",
        "        2. Get style and content features from compute_features\n",
        "        3. Compute the gram_matrix for each style feature\n",
        "        4. set initial image which would get trained to feature the content image \n",
        "        5. Define the optimizer. While you can play with learning_rate, beta, epsilon \n",
        "          Hint: use Adam with learning_rate=5,\n",
        "                              beta_1==0.99, \n",
        "                              epsilon=1e-1\n",
        "        6. Define loss weights style_weight, content_weight\n",
        "        7. Define num_iterations \n",
        "        8. Compute_grad using  tf.GradientTape() to compute gradient\n",
        "      \n",
        "      Note that best_img will be used to autograde based on \n",
        "      similarity of best_img with our provided styled_ref\n",
        "\n",
        "    Args:\n",
        "      content_path, style_path, num_iterations\n",
        "     \n",
        "    Returns:\n",
        "      Tuple: best_img, best_loss ,imgs\n",
        "    '''\n",
        "\n",
        "    model = model_VGG() \n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    # Get the style and content feature representations (from our specified intermediate layers) \n",
        "    style_features, content_features = compute_features(model, content_path, style_path)\n",
        "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "  \n",
        "    # Set initial image\n",
        "    init_image = load_and_process_img(content_path)\n",
        "    init_image = tf.Variable(init_image, dtype=tf.float32)\n",
        "    \n",
        "    # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "    # # Create our optimizer\n",
        "    # opt = tf.optimizers.<FILL IN>\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    # For displaying intermediate images \n",
        "    iter_count = 1\n",
        "  \n",
        "    # Store our best result\n",
        "    best_loss, best_img = float('inf'), None\n",
        "  \n",
        "    # Create a nice config \n",
        "    content_weight=1e3\n",
        "    style_weight=1e-2\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "    cfg = {\n",
        "      'model': model,\n",
        "      'loss_weights': loss_weights,\n",
        "      'init_image': init_image,\n",
        "      'gram_style_features': gram_style_features,\n",
        "      'content_features': content_features\n",
        "    }\n",
        "    \n",
        "    # For displaying\n",
        "    num_rows = 2\n",
        "    num_cols = 5\n",
        "    display_interval = num_iterations/(num_rows*num_cols)\n",
        "    start_time = time.time()\n",
        "    global_start = time.time()\n",
        "  \n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means   \n",
        "  \n",
        "    imgs = []\n",
        "\n",
        "    def compute_grads(cfg):\n",
        "        with tf.GradientTape() as tape: \n",
        "            all_loss = calculate_loss(**cfg)\n",
        "        # Compute gradients wrt input image\n",
        "        total_loss = all_loss[0]\n",
        "        return tape.gradient(total_loss, cfg['init_image']), all_loss\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        # # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code\n",
        "        # grads, all_loss = compute_grads(cfg)\n",
        "        # loss, style_score, content_score = <FILL IN>\n",
        "        # opt.<FILL IN>\n",
        "        # clipped = <FILL IN>\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        init_image.assign(clipped)\n",
        "        end_time = time.time() \n",
        "    \n",
        "        if loss < best_loss:\n",
        "          # Update best loss and best image from total loss. \n",
        "            best_loss = loss\n",
        "            best_img = deprocess_img(init_image.numpy())\n",
        "\n",
        "        if i % display_interval== 0:\n",
        "          # Use the .numpy() method to get the concrete numpy array\n",
        "            plot_img = init_image.numpy()\n",
        "            plot_img = deprocess_img(plot_img)\n",
        "            imgs.append(plot_img)\n",
        "      \n",
        "            print('Iteration: {}'.format(i))        \n",
        "            print('Total loss: {:.4e}, ' \n",
        "                'style loss: {:.4e}, '\n",
        "                'content loss: {:.4e}, '\n",
        "                'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        print('Total time: {:.4f}s'.format(time.time() - global_start))\n",
        "\n",
        "    return best_img, best_loss ,imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSVMx4burydi"
      },
      "source": [
        "best, best_loss ,imgs = run_style_transfer(content_path,style_path, num_iterations=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzJTObpsO3TZ"
      },
      "source": [
        "# select the best image which can pass the similarity test with the provided sample styled image\n",
        "best=imgs[9]\n",
        "Image.fromarray(best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GCIbNxuIjri"
      },
      "source": [
        "best.shape #Just note the shape of output image (345, 512, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JMbRmuOI6SP"
      },
      "source": [
        "Since there can be variety of stylized outputs we have given a wide margin for testing the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlvuDQCFCbhF",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_run_style_transfer",
          "locked": true,
          "points": 14,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#Test stylized images\n",
        "\n",
        "def mse(imageA, imageB):\n",
        "\terr = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        "\terr /= float(imageA.shape[0] * imageA.shape[1])\n",
        "\treturn err\n",
        "\n",
        "a=np.array(Image.open(styled_ref))\n",
        "mse_style_output= mse(a,best)\n",
        "\n",
        "assert mse_style_output<15000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwiZfCW0AZwt"
      },
      "source": [
        "# Visualize outputs\n",
        "We \"deprocess\" the output image in order to remove the processing that was applied to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqTQN1PjulV9"
      },
      "source": [
        "def show_results(best_img, content_path, style_path, show_large_final=True):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    content = resize_img(content_path) \n",
        "    style = resize_img(style_path)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    imshow(content, 'Content Image')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    imshow(style, 'Style Image')\n",
        "\n",
        "    if show_large_final: \n",
        "        plt.figure(figsize=(10, 10))\n",
        "\n",
        "        plt.imshow(best_img)\n",
        "        plt.title('Output Image')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6d6O50Yvs6a"
      },
      "source": [
        "show_results(best, content_path, style_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H55zhvE2X_jJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}