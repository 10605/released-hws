\setlength{\arrayrulewidth}{0.3mm}
\renewcommand{\arraystretch}{2.0}

\section{HW4 Part A: Machine Learning with the Million Song Dataset}
\subsection{Introduction}
%%% what this assignment is about
%%% which part of the homework corresponds to which lecture materials
The goal of this assignment is to gain hands-on experience with training a machine learning model on a large dataset on cloud computing services.

In \textbf{Part A}, you will set up AWS (Amazon Web Services), access, convert the \href{http://millionsongdataset.com/}{Million Song Dataset (MSD)} into a usable format, and run Spark on AWS EMR.  \textbf{START EARLY} because data conversion might take several hours.

Later in Part B, you will do EDA, data cleaning, and modeling in a Jupyter notebook and some (very limited) starter code. At the end, you will have a chance to optimize your pipeline and model. 


\subsection{Logistics}
%This is an individual assignment. 
We provide the code template for Part A in \textit{one} python script for converting the dataset and \textit{one} Jupyter notebook for data loading and preparation. You should follow the instructions in the python script and notebook and implement the missing parts marked with `\texttt{\# YOUR CODE HERE}'. \textbf{Unlike the previous homeworks, this homework gives you more freedom and less guidance so you may need to design your code from scratch and test it by yourself.} 

Note that we will not autograde your code through Gradescope. Instead,  you should submit your code along with your report (including plots, statistics, and short answers). Points will be given according to your answers in the report. We may also refer to your code submission as needed.


\subsection{Getting lab files}
You can obtain the starter code for \textbf{Part A},  \texttt{million\_song\_reader.py} and \texttt{HW4\_partA.ipynb}, after downloading and unzipping \texttt{hw4-a.zip} from \href{https://github.com/10605/released-hws/releases/tag/s21-hw4a}{https://github.com/10605/released-hws/releases/tag/s21-hw4a}.


\subsection{Preparing for submission}
Complete \texttt{million\_song\_reader.py} and \texttt{HW4\_partA.ipynb}. Fill in corresponding written questions using this write-up \texttt{hw4-a.pdf} as a template. 

\subsection{Submission}
\begin{enumerate}
    \item First download your \texttt{million\_song\_reader.py} to your local computer. (You may use \texttt{scp <source> <destination>}). Then download the notebook to your local computer by going to \texttt{File -> Download as -> Notebook (.ipynb)}.
    \item Submit your \texttt{million\_song\_reader.py}, \texttt{hw4.ipynb}, and this write-up \texttt{hw4.pdf} to Gradescope.
\end{enumerate}

\clearpage
    
\section{Part A: Data Conversion and Preparation}
To get ready for Part B, you will need to set up your AWS account and redeem your \$50 credits. You are also expected to access the MSD and transform it from \texttt{h5} format to \texttt{csv} while keeping only the features we need. Finally, you will set up Spark on an AWS EMR cluster and run some very preliminary analysis over the converted dataset.

Again, \textbf{PLAN TO START EARLY}. One reason is that the entire MSD is 280GB and running data conversion on it might take a few hours. It will be frustrating if your code runs for several hours before deadline then aborts due to a tiny bug and you have to re-run. 

The other reason is that you may run into a few roadblocks when working with a large dataset on the cloud, especially if this is your first time performing such an exercise. Although we have included several tips in this writeup, you may still encounter some unexpected problems. Luckily, search engines are always your friend. If you have problems with something, someone elsewhere might have encountered the same problem! We recommend that you first try searching for solutions to your problems online (as this will likely be the fastest route to an answer), and if you're still stuck to then post on Piazza and / or come to office hours.

\subsection{Setting up AWS}
\href{https://aws.amazon.com/}{Amazon Web Service (AWS)} is one of the most comprehensive cloud computing platforms. Although there are many other options for cloud computing (e.g. Azure, GCP), you should use AWS for this particular assignment. This makes us supporting the entire class much easier.
\subsubsection{Create AWS account \& redeem credits}
Create an AWS account if you don't already have one. You will receive an email from the course staff containing a \$50 coupon code on AWS. You can redeem it at \texttt{Billing Dashboard -> Credits}. \textbf{Note that this \$50 coupon is meant to cover your costs for ALL OF HOMEWORK 4 (i.e., for both Parts A and B}).
\subsubsection{Cost management}
Make sure you manage your cost while doing this assignment. To do this effectively, we highly recommend you learn about \href{https://aws.amazon.com/ec2/pricing/on-demand/}{EC2 pricing} and \href{https://aws.amazon.com/s3/pricing/}{S3 pricing}, and \textbf{ \href{https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html\#create-cost-budget}{create an AWS Budget} of ~35 dollars so that you are alerted when you are close to your budget limit.} Technically, the course staff will not be responsible for covering extra charges incurred beyond the supplied coupon.

\subsubsection{AWS S3}
You have to \href{https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html}{create a S3 bucket} in order to store your converted MSD into it. 
\subsubsection{IAM Role}
To have read/write access of S3, you also have to create a role with the \texttt{AmazonS3FullAccess} policy attached under \texttt{IAM -> Roles -> Create role} (\href{https://console.aws.amazon.com/iam/home#/roles}{here}).

\subsubsection{Configure and launch EC2}
Configure and launch EC2 instances to develop and run your data conversion. Here are some guidelines (note: we will cover this process in detail in lecture on Wednesday 3/17):
\begin{itemize}
    \item Choose the default image (i.e. something like \texttt{Amazon Linux 2 AMI (HVM)}). Then choose \texttt{t2.micro} or \texttt{t2.medium}. Note that \texttt{t2.micro} can be used for development, but might not have enough memory to process your entire dataset.
    \item Select a subnet starting with \textbf{``us-east-1"}. Remember your choice since you have to select the same subnet for the volume you are going to create.
    \item Choose the IAM role you just created.
    \item Keep options default and move on. You will need to select a security group (this can be changed after creating the instance). You may select the ``default" security group and add an inbound rule to it for the \texttt{SSH} service with \texttt{source} set to be \texttt{Anywhere}.
    \item Create a key-pair if you haven't already.
\end{itemize}

The image might not come pre-installed with Python 3 so you could do so via \texttt{\$ sudo yum install python37}. You should also install packages you import in the python file. Use \texttt{pip3} with the \texttt{--user} flag to avoid \texttt{sudo}.

Again, cost management is key. You could launch a single \texttt{t2.micro} while you are developing to save cost. You should launch up to two \texttt{t2.medium} instances while you run the actual conversion. You should stop (\textbf{not} terminate, see distinction \href{https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html}{here}) your instances when you are not using them for a while. When an instance is stopped, you will only be charged EBS storage fees of your data but not instance running fees. You should terminate them \textbf{after you have downloaded your code} when you are done.

\subsubsection{Create MSD volume}
Create a volume from the AWS \href{https://aws.amazon.com/datasets/million-song-dataset/}{MSD snapshot}. Select the same ``subnet" as the EC2 instance's, (an example subnet might be ``us-east-1d"). \textbf{Remember the subnet must start with ``us-east-1", or the snapshot of MSD cannot be found}. Attach and \href{https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html}{mount} the MSD volume to the EC2 instance. We will run through this process in more details in lecture. Remember to delete this volume after you are done with this homework.

\subsubsection{Credentials}
\textbf{You may not need to do this in this homework}. But if you want to use the AWS command line tool, you may have to configure credential files in order to let your program access AWS services. The easiest way would be to run \texttt{\$ aws configure}. See \href{https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html}{Configuration and credential file settings} for details.

\subsubsection{Development on AWS}
You could use whatever IDE/editor you are comfortable with. For starters with remote development, you could try Visual Studio Code with the Remote-SSH extension (see details \href{https://code.visualstudio.com/docs/remote/ssh}{here}) or Vim.

\clearpage

\subsection{Million Song Reader}
After completing the above steps, we are ready to operate on the dataset and convert it into a desirable format. The starter code is provided in \texttt{million\_song\_reader.py}. Upload this file to your instance. (You may use \texttt{scp <source> <destination>})

General guidance and tips of converting the data are the following. Please refer to the comments in the \texttt{.py} file for detailed instructions. You don't necessarily have to follow our code template if you think it's easier to write your conversion script from scratch.
\begin{itemize}
    \item You should start by understanding how to access the fields in \texttt{h5} format (instructions are given in the comments under the function \texttt{process\_h5\_file}). 
    \item For you to know what features we are extracting from the original dataset, the complete list of fields is provided \textit{at the top} in the python file. We want to include these fields in the resulting \texttt{csv} files.
    
    Take a look at \href{http://millionsongdataset.com/pages/field-list/}{MSD's full field list}. Briefly explain why we include this specific subset of fields but not the others.  (Hint: think about what category of features we omit and why that might be a sensible choice.)\grade{15}
    
    \begin{tcolorbox}[fit,height=5cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %solution
    \end{tcolorbox}
    
    \item You should discard all records with \texttt{song\_hotttnesss} field being \texttt{NaN}. (After all, this is what we are predicting for!)
    
    \item The dataset volume is attached to the instance and therefore can be access ``locally" (we can treat it as local) from the instance. Then we need to generate temporary \texttt{csv} files in batches and upload them to S3. 
    
    
    \item To speed up the process, you may want to split the jobs into different proportions and distribute them to different threads or different machines to allow parallel computation. 
    
    The starter code for setting up the command-line arguments is provided. After you finished, you will be able to run \texttt{python million\_song\_reader.py 4 0}, which means split the job into 4 parts and this machine will work on job number 0. You can run this code on a single machine (multi-thread) or multiple machines to speed up the conversion.
    
    Again, the specific parameters are up to you. From our experience, running conversion to saturate performance on two \texttt{t2.medium} instances will do the job within 2 hours. You should experiment with the parameters while monitoring machine resource utilization (e.g. CPU utilization by \texttt{top} or \texttt{htop} and disk I/O by \texttt{iostat}) to optimize for cost. 
    \\ 
    \clearpage
    Briefly describe your final configuration and the approaches you took to arrive at it. \grade{20}
    
    \begin{tcolorbox}[fit,height=6cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %solution
    \end{tcolorbox}
    
    Report the time and cost the conversion step took in total. \grade{10}
    
    \begin{tcolorbox}[fit,height=5cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %solution
    \end{tcolorbox}
    
    Briefly describe any pitfalls or obstacles you faced during the data conversion process and how you overcame them. \grade{10}
    
    \begin{tcolorbox}[fit,height=5cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %solution
    \end{tcolorbox}
    
\end{itemize}

\clearpage

\subsection{EMR and Spark}

With our data ready in S3, it's now time to configure and create an EMR (Elastic MapReduce) cluster and run Spark, starting from the notebook \texttt{HW4\_partA.ipynb}. Include at least Hadoop, JupyterHub, and Spark in your cluster software configuration. Set \texttt{maximizeResourceAllocation} to true (see \href{https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html}{here}) in software settings. Select your EC2 key pair.

You'll have to do \href{https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel-local.html}{ssh port forwarding} (recommended) or attach additional security groups/inbound rules to the master node in order to access the JupyterHub web interface. This step could be tricky and we will cover how to do this in detail in lecture. Then, log in to jupyter (learn about login credentials \href{https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub-user-access.html}{here}), upload your notebook, and start working!

Again, cost management is key. Because we might be running a cluster of machines, this could easily blow up your budget. We recommend using at most 1 Driver and 1 Core of type \texttt{m5.xlarge} while developing and debugging on a subset of MSD. You could scale this up to multiple Core workers when doing the final run. You may also utilize \href{https://aws.amazon.com/ec2/spot/}{AWS spot instances} to save cost during development.

Note that, although EMR is made up of EC2 instances, unlike EC2, you cannot stop an EMR cluster - you can only terminate it. See \href{https://forums.aws.amazon.com/thread.jspa?threadID=149772}{here} for why this is the case. You should plan your strategy accordingly. \textbf{Do not forget to download your code} before you terminate a cluster when you are done.

\subsection{Notebook: Data Loading and Preparation}
\begin{enumerate}[label=(\alph*)]
    \item \grade{15}List the 19 numeric features:\hbox{}
    
        \begin{tcolorbox}[fit,height=4cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %solution
        \end{tcolorbox}
        
    \item \grade{30}Include the output of the final cell  (make sure you run on the entire dataset!):\hbox{}
    
        \begin{tcolorbox}[fit,height=6cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %solution
        \end{tcolorbox}
        
\end{enumerate}

    

