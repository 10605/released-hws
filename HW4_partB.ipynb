{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Machine Learning with Large Datasets\n",
    "## Homework 4 - Machine Learning at Scale: Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation from HW4 Part A. Note that we've included code that does data loading and preparation for you. You could take a brief look to learn about how to specify a schema when loading data, or just run them all and start from \"Part B Begins\".\n",
    "\n",
    "Note that we will not be autograding this notebook because of the open-ended nature of it (although you will have to submit this notebook). To make grading easier and to learn about your thought process, throughout the notebook, we include questions you have to anwswer in your writeup. Whenver this happens, there is a ✰ symbol.\n",
    "\n",
    "### 0. Start a Spark Session and Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are highly recommended to select the \"PySpark\" kernel instead of python kernel,\n",
    "# Otherwise you need to modify this cell to get pyspark working.\n",
    "\n",
    "spark\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f'num executors: {sc.getConf().get(\"spark.executor.instances\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this assignment, you will be generating plots. `Matplotlib` and other useful Python libraries do not come pre-installed on the cluster. Therefore, you will have to ssh into your master node (think about why it should be the master) using your keypair created earlier and install `matplotlib`. You might have to do this later again for other libraries you use, e.g. `Pandas`.\n",
    "\n",
    "Run the below cell to ensure you installation was successful. If an error occurs, you might want to double check your installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Preparation\n",
    "\n",
    "Earlier, we have extracted relevant features from and converted format of the full raw Million Song Dataset. We now want to load our converted dataset from the S3 Storage.\n",
    "\n",
    "Note that although you can load all chunks of the dataset using `*`, we recommend you only load in a subset while developing so that processing takes shorter time when you are just verifying your ideas.\n",
    "\n",
    "**IMPORTANT**: to make grading consistent and to make those who failed to finish Part A still able to work on the rest of this homework, you are asked to use **our S3 bucket** (instead of the one you created in Part A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType, StructField, StructType\n",
    "\n",
    "schema = StructType([StructField('song_hotttnesss', DoubleType(), True),\n",
    "                       StructField('artist_familiarity', DoubleType(), True),\n",
    "                       StructField('artist_hotttnesss', DoubleType(), True),\n",
    "                       StructField('artist_id', StringType(), True),\n",
    "                       StructField('artist_latitude', DoubleType(), True),\n",
    "                       StructField('artist_location', StringType(), True),\n",
    "                       StructField('artist_longitude', DoubleType(), True),\n",
    "                       StructField('artist_name', StringType(), True),\n",
    "                       StructField('title', StringType(), True),\n",
    "                       StructField('danceability', DoubleType(), True),\n",
    "                       StructField('duration', DoubleType(), True),\n",
    "                       StructField('end_of_fade_in', DoubleType(), True),\n",
    "                       StructField('energy', DoubleType(), True),\n",
    "                       StructField('key', DoubleType(), True),\n",
    "                       StructField('key_confidence', DoubleType(), True),\n",
    "                       StructField('loudness', DoubleType(), True),\n",
    "                       StructField('mode', DoubleType(), True),\n",
    "                       StructField('mode_confidence', DoubleType(), True),\n",
    "                       StructField('start_of_fade_out', DoubleType(), True),\n",
    "                       StructField('tempo', DoubleType(), True),\n",
    "                       StructField('time_signature', DoubleType(), True),\n",
    "                       StructField('time_signature_confidence', DoubleType(), True),\n",
    "                       StructField('artist_terms', StringType(), True),\n",
    "                       StructField('artist_terms_freq', StringType(), True),\n",
    "                       StructField('artist_terms_weight', StringType(), True),\n",
    "                       StructField('year', DoubleType(), True)]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_subset = True\n",
    "s3_bucket_name = '10605-group37-s3'\n",
    "\n",
    "if load_subset:\n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"false\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(f\"s3://{s3_bucket_name}/processed/A_1.csv\")\n",
    "else:\n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"false\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(f\"s3://{s3_bucket_name}/processed/*.csv\")\n",
    "\n",
    "print('loaded {} records'.format(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the `df` we just created by running the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>------------------- Part B Begins ------------------- </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis\n",
    "\n",
    "Now we have our data mostly ready. It's a good time to take some deeper look to better understand what we are dealing with here.\n",
    "\n",
    "First, show summary statistics of the features using `.summary()`. Hint: because we have many features, the output might be too long for a single line. The simplest way to resolve this mess is to copy paste the output to some editor (e.g. vscode) and check the result. Another way is you could select only a few features to print at a time so that things could fit into one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the statistics, `danceability` and `energy` appear strange. ✰2.1(a) Explain why these features seem problematic in your writeup.\n",
    "\n",
    "Now we would like to make some histogram plots to inspect the distribution of feature values. \n",
    "\n",
    "✰2.1(b) Plot histograms for `'song_hotttnesss', 'artist_familiarity', 'artist_hotttnesss', 'duration', 'tempo', 'year'`. \n",
    "\n",
    "Note that one of these features may appear weird. \n",
    "\n",
    "✰2.1(c) Explain what is weird about `year`'s distribution and what might cause this. Describe how you could filter `year` to make its histogram look more balanced. Hint: Choose a threshold and filter the `year` values.\n",
    "\n",
    "✰2.1(d) Do it and include the new plot of this feature into your writeup.\n",
    "\n",
    "Note: you may want to use the spark magic commands `%matplot plt` to show the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some scatter pair plots would also be interesting to see the correlation between features. ✰2.1(e) Include the plots of the following pairs of features and describe your findings (e.g. what patterns you observe) in your writeup.\n",
    "- `song_hotttnesss` against `artist_familiarity`\n",
    "- `artist_latitude` against `artist_longitude`\n",
    "- `song_hotttnesss` against `year`\n",
    "\n",
    "Plotting all data points might explode matplotlib. Think about what simple technique you could use to visualize large datasets while retaining data distribution. ✰2.1(f) In your writeup, briefly describe what you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "\n",
    "From 2, you should have had a basic understanding of the dataset. In this cleaning step, we are dropping `energy` and `danceability` features. \n",
    "\n",
    "✰2.2(a) In your writeup, justify why we are doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we are dropping `year` values that are less or equal to 1920. Print out the number of samples before and after dropping these rows. \n",
    "\n",
    "✰2.2(b) In your writeup, compare these two numbers and explain the advantages and potential problem of doing this step. What other techniques could you use to potentially do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do an NaN/null check to see if there are other problematic features. Print the number of entries that contain NaN/null for each feature. \n",
    "\n",
    "Hint: use pyspark sql `count(when(isnan(...)))` to count total NaN entries for a feature. Replace `isnan` with `isnull` to count null entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count na\n",
    "from pyspark.sql.functions import isnan, when, count, isnull\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see two features that contain (a lot of) `null` in them. ✰2.2(c) Which two are they? \n",
    "\n",
    "For simplicity, we are dropping all records with `null` in these two features. Note that this drops a significant proportion of our dataset. Since we have a lot of data to fit some rather simple models, this might be ok. \n",
    "\n",
    "However, do note that this is not usually desirable in the real-word because we always want to retain as much data as possible, especially when training larger, more complex models. ✰2.2(d) In your writeup, explain what possible techniques could you employ to deal with this situation. Discuss the pros and cons of your proposed solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✰2.2(e) Finally in your writeup, report the percentage of records that survived our very aggressive data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Baseline\n",
    "\n",
    "Now we have the data (almost) ready to do some preliminary modeling! \n",
    "\n",
    "We will be treating this problem as a classification problem, i.e. given some features, predict whether a song will be popular. We define a song to be popular if its `song_hotttnesss` value is above average. \n",
    "\n",
    "✰2.3(a) Explain in your writeup why treating this as a classification problem (instead of a regression problem) might be a sensible choice. \n",
    "\n",
    "Then, assign labels to the dataset with the above definition of \"popularity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "# assign labels\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how balanced the two classes are. ✰2.3(b) Report what percentage of songs are assigned the \"popular\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As how we've been dealing with the `year` feature in earlier assignments, we will shift the feature so that it starts from 0.\n",
    "\n",
    "✰2.3(c) Explain why we want to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift years\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's concatenate all features (using VectorAssembler) into a feature vector and scale it. \n",
    "\n",
    "✰2.3(d) Explain what scaling means and why we want to perform scaling before the learning step.\n",
    "\n",
    "Note: we will only be using the numeric features for now. Excluding `song_hotttnesss` and the two features we dropped in the data cleaning step, there should be 19-3=16 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling vector\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the resulting feature vector is of expected length 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change your variable/column names accordingly\n",
    "l = df_features.select('features').take(1)[0].features\n",
    "print('vector len:',len(l))\n",
    "assert len(l) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are really ready to fit some models. \n",
    "\n",
    "First, do a train-test split on the dataset, with test ratio 0.2 and seed 10605."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to select a metric to evaluate our models on. For classification, potential choices include accuracy and AUC. ✰2.3(e) In your writeup, explain the difference between these two metrics and when AUC might be more useful than accuracy.\n",
    "\n",
    "We will be going with AUC here. Instantiate an AUC Evaluator in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting two models, logistic regression, and random forest, in the mandatory part. You have the chance go with fancier models in the last section to achieve higher accuracy to earn additional points.\n",
    "\n",
    "Train a LR and a RF model with default hyperparameters. ✰2.3(f) Calculate the train and test AUC of both models and report them in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Featurization: Bag-of-Words and TF-IDF\n",
    "\n",
    "In our list of features, we have two features that contain text data, namely `title` and `artist_terms`. In the entity resolution task of HW1, we've seen how TF-IDF could help us featurize textual data. Let's do that here as well. [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) is yet another technique (arguably simpler than TF-IDF) to featurize text features.\n",
    "\n",
    "\n",
    "To get yourself familiar with both in Spark, let's treat titles and artist terms as mini documents and compute TF-IDF for `title` and BoW for `artist_terms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf on title\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "TF_IDF_NUM_FEATS = 5 # HashingTF(..., numFeatures=TF_IDF_NUM_FEATS)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the resulting TF-IDF feature is indeed a vector of length `TF_IDF_NUM_FEATS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change variable/column name to yours\n",
    "col_name = 'title_features'\n",
    "head = df_tf_idf.select(col_name).head() \n",
    "print(head)\n",
    "assert type(head[col_name]) == pyspark.ml.linalg.SparseVector\n",
    "assert len(head[col_name]) == TF_IDF_NUM_FEATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `artist_terms` should be of type string array but we have not yet interpreted from a string literal. Let's do that now. \n",
    "\n",
    "Hint: start by defining a UDF to convert a single string literal to type `T.ArrayType(T.StringType())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check things indeed worked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to your variable/column names\n",
    "col_name = 'artist_terms_arr'\n",
    "head = df_tags_arr.select(col_name).head()\n",
    "print(head)\n",
    "assert type(head[col_name]) == list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform BoW on the array feature you just created.\n",
    "\n",
    "✰2.4(a) In your writeup, explain what the `vocabSize` hyperparameter means in the context of Bag-of-Words. Remember you can tune this later in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW on artist_terms\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "BOW_VOCAB_SIZE = 10 \n",
    "BOW_MIN_DF = 2\n",
    "# CountVectorizer(..., vocabSize=BOW_VOCAB_SIZE, minDF=BOW_MIN_DF)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the result of BoW is indeed a vector of length `BOW_VOCAB_SIZE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change variable/column name to yours\n",
    "col_name = 'artist_terms_bow'\n",
    "head = df_final.select(col_name).head() \n",
    "print(head)\n",
    "assert type(head[col_name]) == pyspark.ml.linalg.SparseVector\n",
    "assert len(head[col_name]) == BOW_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✰2.4(b) Other than featurizing texts, what other feature engineering would you do on the dataset? Briefly describe one in your writeup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the new feature columns ready, let's assemble and scale our features once again as we did to prepare for training. \n",
    "\n",
    "This time, we should have 16+2=18 features with the two being TF-IDF and BoW features we just created. The total length of the resulting feature vector should be 31. ✰2.4(c) Explain where this number (31) comes from in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling vector\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df_scaled_features.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the feature vector is of expected length 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change your variable/column names accordingly\n",
    "l = df_scaled_features.select('features').take(1)[0].features\n",
    "print('vector len:',len(l))\n",
    "assert len(l) == 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modeling with New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit LR and RF on our new data. As before, first do a train-test split with test ratio 0.2 and seed 10605."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the LR and RF model with default hyperparameters. ✰2.5(a) Evaluate train and test AUC for each model and report them in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that AUC is the area under the ROC curve. Now, plot the ROC curves for the four models (including two baselines) in **one single plot**. \n",
    "\n",
    "✰2.5(b) In your writeup:\n",
    "- Include the plot (with legends)\n",
    "- Explain how the ROC curve is derived and what it measures\n",
    "- Explain, from the ROC curves, how do you discover which models are performing better than others, and in this case, which model performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Do Your Best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all previous sections in this assignment and other assignments so far, we have almost specified everything you should do. You might be (and should be!) bored by now. This section gives you a chance to do whatever you want to improve the prediction AUC. \n",
    "\n",
    "You can do better data preprocessing, feature engineering, fit fancier models, perform hyperparameter tuning, etc. \n",
    "\n",
    "After you are satisfied with your model, ✰2.6 in your writeup, report \n",
    "- the hyperparameters,\n",
    "- train and test AUC of your optimized model, and \n",
    "- the approach you took on top of the specified instructions to obtain this better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have fun!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Don't forget to answer the reflection question on the writeup! ✰2.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
