\documentclass{exam}

%------------------------ packages ------------------------%
\usepackage{amsmath,amsfonts,amsthm,amssymb,amsopn,bm}
\usepackage{pythontex}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[paper=letterpaper,margin=1in,includeheadfoot,footskip=0.25in,headsep=0.25in]{geometry}
\usepackage{url}
\usepackage[usenames,dvipsnames]{color}
\usepackage[pdfborder={0 0 1},colorlinks=true,citecolor=black,plainpages=false]{hyperref}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage{pdfpages,bbm}
\usepackage{enumitem}


%------------------------ math ------------------------%
\newcommand{\R}{\mathbb{R}} % real domain
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\U}{\mathbf{U}}
\renewcommand{\L}{\mathbf{L}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\mD}{\mathbf{D}}



%------------------------ exam class macros ------------------------%
\checkboxchar{$\Box$}
\renewcommand{\checkboxeshook}{
  \settowidth{\leftmargin}{W.}
  \labelwidth\leftmargin\advance\labelwidth-\labelsep
}

\newcommand{\grade}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}


\begin{document}

\title{Homework 1 Written Assignment}
\author{\Large \bf 10-605/10-805: Machine Learning with Large Datasets}
\date{{\bf Due Tuesday, September 15th at 1:30:00 PM Eastern Time}}
\maketitle

\noindent Submit your solutions via Gradescope, \textbf{with your solution to each subproblem on a separate page}, i.e., following the template below.  Note that Homework 1 consists of two parts: this written assignment, and a programming assignment. Each part (written and programming) is worth 50\% of your total HW1 grade.

\newpage

\section{Sparse PCA} Sparse PCA is a variant of PCA that can be used to better interpret which of the $k$ features are most important for the various principal components. In sparse PCA, the first principal component $\bm{p}$ is the solution to the following constrained optimization problem:
\vspace{-.5em}
\begin{equation}\label{eq:sparse_pca}
\begin{aligned}
& \min_{\bm{p}}
& & \bm{p}^T \mathbf{C_X} \bm{p} \\
& \text{subject to}
& & ||\bm{p}||_2 = 1 \text{ and } \quad ||\bm{p}||_0 \leq s,
\end{aligned}
\end{equation}
where $\mathbf{X}$ is an $n \times k$ matrix of $n$ data points. The L0 norm $||\bm{p}||_0$ represents the number of non-zero elements in the vector $\bm{p}$, and we want this to be at most $s$ for $1 \leq s \leq k$.


\begin{enumerate}[label=(\alph*)]

    \item \grade{10} The second constraint of the above optimization problem,  $||\bm{p}||_0 \leq s$,  is not convex. Show this by proving that the L0 norm $||\bm{p}||_0$ is not a convex function.
 
    \newpage
    \item \grade{10} Being a non-convex problem, \eqref{eq:sparse_pca} is hard to solve directly. Instead, we can use the following brute-force optimization method. Fix a set $S$, which is a size $s$ subset of $\{1, 2, \dots, k\}$ corresponding to $s$ locations of non-zero elements in $\bm{p}$. For each $S$ we solve the following convex problem:
    \vspace{-.5em}
     \begin{equation}\label{eq:sparse_pca_fix_supp}
         \begin{aligned}
         & \min_{\bm{p}}
         & & \bm{p^T C_X p} \\
         & \text{subject to}
         & & ||\bm{p}||_2 = 1 \text{ and } \;\; p_i = 0 \text{ for } i \not \in S,
         \end{aligned}
     \end{equation}
    where $p_i$ is the $i$-th element of $\bm{p}$. To solve the original problem \eqref{eq:sparse_pca}, we can solve \eqref{eq:sparse_pca_fix_supp} for all possible sets $S$ and choose the solution that gives the minimum objective value.

    How many different sets $S$ will we have to iterate over in the above method? Why is this brute-force method computationally expensive for high-dimensional data?

   \newpage    
   \item \grade{5} Instead of the brute-force method suggested in (b) which yields an exact solution, we may want to consider approximate solutions that do not necessarily satisfy $||\bm{p}||_0 \leq s$, but impose some sparsity in $\bm{p}$. Do you have any suggestions for an approximate solution which can be more computationally tractable?

\end{enumerate}

\newpage


\section{Laplacian Eigenmaps}
Several non-linear methods for dimensionality reduction assume that high-dimensional data lie on or near a low-dimensional non-linear manifold
embedded in the input space. 
These so-called manifold learning techniques  aim to learn this manifold structure. Global methods, e.g,. Isomap, aim to preserve distances along the manifold between all pairs of data points. In contrast, local methods, like Laplacian Eigenmaps focus only on preserving local neighborhood relationships in the high-dimensional space. In particular, given a dataset of $n$ data points, Laplacian Eigenmaps works by first finding $t$ nearest neighbors for each point; then constructing $\W$, the sparse, symmetric $n \times n$ nearest neighborhood adjacency matrix, where $\W_{ij} =
\textrm{exp}\big (-||\x_i-\x_j||_2^2 / \sigma^2 \big)$ if
($\x_i,\x_j$) are neighbors\footnote{To ensure that $\W$ is symmetric, we define ($\x_i,\x_j$) as neighbors if either data point is a $t$ nearest neighbor of the other.}, $0$ otherwise, and $\sigma$ is a scaling
parameter; and finally, finding the $r$-dimensional representation that minimizes the following objective:
\begin{equation} 
\label{eq:Laplacian}
\Y=\argmin_{\Y'}  \sum_{i,j}\W_{ij}||\y'_i - \y'_j||_2^2 \,,
\end{equation} 
subject to appropriate constraints on $\Y'$. Note that $\Y' \in \mathbb{R}^{r \times n}$ and $\y'_i \in \mathbb{R}^r$ is the $i$th column of $\Y'$.
\begin{enumerate}[label=(\alph*)]
\item \grade{5} Describe in words how the objective in Eq~\eqref{eq:Laplacian} aims to preserve local neighborhood relationships.  What is the role of the weight matrix $\W$?

\newpage
\item  \grade{10} Assume we seek a one-dimensional representation $\y$, i.e., $r=1$. Show that \eqref{eq:Laplacian} is equivalent to 
\begin{equation}
\label{eq:graph_laplacian}
\y=\argmin_{\y'} \y'^\top \L \y',
\end{equation}
where $\mD$ is the diagonal matrix with $\mD_{ii} = \sum_j \W_{ij}$, and $\L = \mD - \W$ is the graph Laplacian.


\newpage
\item \grade{10} Without any constraints on $\y'$, what is the solution to Eq.~\eqref{eq:graph_laplacian}?  Is this a good solution?  If so, why?  If not, how might the problem be appropriately constrained? 


\end{enumerate}




\end{document}
